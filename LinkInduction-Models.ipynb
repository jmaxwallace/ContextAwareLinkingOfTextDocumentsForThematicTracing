{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30887fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "import regex as re\n",
    "import requests\n",
    "import json\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import requests\n",
    "import time\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "import random\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import joblib\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from umap import UMAP\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2076f9c",
   "metadata": {},
   "source": [
    "# Graph Creation\n",
    "\n",
    "## Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"docs_with_subtopics.db\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * from docs_with_subtopics\", con)\n",
    "con.close()\n",
    "\n",
    "df['topic_list'] = df['topic_list_str'].apply(json.loads)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"topic_counts.db\")\n",
    "\n",
    "topic_counts = pd.read_sql_query(\"SELECT * from topic_counts\", con)\n",
    "con.close()\n",
    "\n",
    "topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96137957",
   "metadata": {},
   "source": [
    "## Check Subtopic Co-Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_topic_matrix = (topic_counts\n",
    "                      .pivot(index='title', columns='topic_number', values='topic_count')\n",
    "                      .fillna(0)\n",
    "                      .astype(int)\n",
    ")\n",
    "binary_topic_matrix = (binary_topic_matrix > 0).astype(int)\n",
    "\n",
    "co_occurrence_matrix = binary_topic_matrix.T.dot(binary_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa14a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix.index.name = None\n",
    "co_occurrence_matrix.columns.name = None\n",
    "\n",
    "co_occur_pairs = (\n",
    "    co_occurrence_matrix.stack()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "co_occur_pairs.columns = ['topic_1','topic_2','count']\n",
    "\n",
    "co_occur_pairs = co_occur_pairs[co_occur_pairs['topic_1'] < co_occur_pairs['topic_2']]\n",
    "\n",
    "top_pairs = co_occur_pairs.sort_values(\"count\", ascending=False)\n",
    "\n",
    "top_pairs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = co_occurrence_matrix.sum(axis=1)\n",
    "\n",
    "self_counts = np.diag(co_occurrence_matrix.values)\n",
    "\n",
    "co_occurs_with_others = row_sums - self_counts\n",
    "\n",
    "isolated_topics = co_occurs_with_others[co_occurs_with_others==0].index.tolist()\n",
    "\n",
    "print(\"Topics with no co-occurrence:\", isolated_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(co_occurs_with_others.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_co_occ_counts(topic_counts, binary_matrix, co_occurrence_series):\n",
    "    unique_topics = topic_counts['topic_number'].unique()\n",
    "    mismatches = []\n",
    "    \n",
    "    for topic in tqdm(unique_topics, desc='Verifying topics'):\n",
    "        if topic not in binary_matrix.columns:\n",
    "            print(f\"topic {topic} not in binary matrix. Skipping\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        docs = binary_matrix[binary_matrix[topic]==1]\n",
    "        \n",
    "        manual_total = (docs.sum(axis=1) - 1).sum()\n",
    "        \n",
    "        stored_value = co_occurrence_series.get(topic,None)\n",
    "        \n",
    "        if stored_value != manual_total:\n",
    "            mismatches.append({\n",
    "                \"topic\": topic,\n",
    "                \"stored\": stored_value,\n",
    "                \"manual\": manual_total,\n",
    "                \"difference\" : manual_total-stored_value\n",
    "            })\n",
    "    \n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66736e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches = verify_co_occ_counts(topic_counts, binary_topic_matrix, co_occurs_with_others)\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "\n",
    "print(mismatches_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0b184",
   "metadata": {},
   "source": [
    "## Prepare Graph Inputs\n",
    "\n",
    "### Subtopic Embedding Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subtopic_embeddings.pkl\", \"rb\") as f:\n",
    "    subtopic_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.stack([emb.cpu().numpy() for emb in subtopic_embeddings.values()])\n",
    "\n",
    "subtopic_ids = list(subtopic_embeddings.keys())\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame(similarity_matrix, index=subtopic_ids, columns=subtopic_ids)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = similarity_df.mean(axis=None)\n",
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8060eb0",
   "metadata": {},
   "source": [
    "### Compute Directed Edge Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topic_counts = co_occurrence_matrix.values.diagonal()\n",
    "topic_labels = co_occurrence_matrix.index.tolist()\n",
    "\n",
    "topic_count_dict = dict(zip(topic_labels, total_topic_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "\n",
    "for i, topic_i in enumerate(topic_labels):\n",
    "    for j, topic_j in enumerate(topic_labels):\n",
    "        if i == j:\n",
    "            continue # Skip Diagonals\n",
    "        \n",
    "        co_occur = co_occurrence_matrix.loc[topic_i,topic_j]\n",
    "        count_i = topic_count_dict[topic_i]\n",
    "        count_j = topic_count_dict[topic_j]\n",
    "        \n",
    "        if count_i == 0 or count_j == 0:\n",
    "            continue\n",
    "            \n",
    "        # Directional Probabilities\n",
    "        p_ij = co_occur / count_i\n",
    "        p_ji = co_occur / count_j\n",
    "        \n",
    "        # Keep only stronger direction\n",
    "        if p_ij > p_ji:\n",
    "            edges.append((topic_i, topic_j, p_ij))\n",
    "        elif p_ij < p_ji:\n",
    "            edges.append((topic_i, topic_j, p_ji))\n",
    "        else: # Bidirectional edge if probabilities are equal\n",
    "            edges.append((topic_i, topic_j, p_ji))\n",
    "            edges.append((topic_i, topic_j, p_ij))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe432a8",
   "metadata": {},
   "source": [
    "## Graph Construction - MultiDiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = df[['title', 'cluster', 'topic_list']]\n",
    "similarity_threshold = 0.5\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "#Nodes\n",
    "for _, row in graph_df.iterrows():\n",
    "    G.add_node(\n",
    "        row['title'],\n",
    "        cluster=row['cluster'],\n",
    "        subtopics=row['topic_list']\n",
    "    )\n",
    "    \n",
    "# Subtopic > Document mapping\n",
    "topic_to_docs = defaultdict(set)\n",
    "for _, row in graph_df.iterrows():\n",
    "    for topic in row['topic_list']:\n",
    "        topic_to_docs[topic].add(row['title'])\n",
    "        \n",
    "# Lookup for edge weights\n",
    "subtopic_edge_dict = {\n",
    "    (src, tgt): weight\n",
    "    for src, tgt, weight in edges\n",
    "}\n",
    "\n",
    "\n",
    "titles = graph_df['title'].tolist()\n",
    "\n",
    "for doc1, doc2 in combinations(graph_df['title'], 2):\n",
    "    subs1 = set(G.nodes[doc1]['subtopics'])\n",
    "    subs2 = set(G.nodes[doc2]['subtopics'])\n",
    "    \n",
    "    for s1 in subs1:\n",
    "        for s2 in subs2:\n",
    "            if s1 == s2:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            try:\n",
    "                sim = similarity_df.loc[s1,s2]\n",
    "                if sim < similarity_threshold:\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                continue # Skip missing value\n",
    "            \n",
    "            # Check Direction\n",
    "            if (s1, s2) in subtopic_edge_dict:\n",
    "                weight = subtopic_edge_dict[(s1,s2)]\n",
    "                G.add_edge(doc1, doc2, weight=weight, sim=sim, subtopic_pair=(s1, s2))\n",
    "            elif (s2, s1) in subtopic_edge_dict:\n",
    "                weight = subtopic_edge_dict[(s2, s1)]\n",
    "                G.add_edge(doc2, doc1, weight=weight, sim=sim, subtopic_pair=(s2,s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b8bd3",
   "metadata": {},
   "source": [
    "## Comparing clustering in graph to document K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a60db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = {node: G.nodes[node]['cluster'] for node in G.nodes()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as co\n",
    "\n",
    "undirected_G = nx.Graph()\n",
    "undirected_G.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "for u, v, data in G.edges(data=True):\n",
    "    current_weight = undirected_G.get_edge_data(u, v, default={'weight':0}).get('weight', 0)\n",
    "    undirected_G.add_edge(u, v, weight=current_weight + data.get('weight', 1))\n",
    "    \n",
    "print(f\"Converted to undirected Graph with {undirected_G.number_of_nodes()} nodes and {undirected_G.number_of_edges()} edges.\")\n",
    "\n",
    "partition = co.best_partition(undirected_G, weight='weight')\n",
    "\n",
    "graph_clustering_labels = partition\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "kmeans_labels = {node: G.nodes[node]['cluster'] for node in G.nodes()}\n",
    "\n",
    "common_nodes = sorted(list(set(kmeans_labels.keys()) & set(graph_clustering_labels.keys())))\n",
    "\n",
    "if common_nodes:\n",
    "    kmeans_labels_ordered = [kmeans_labels[node] for node in common_nodes]\n",
    "    graph_labels_ordered = [graph_clustering_labels[node] for node in common_nodes]\n",
    "    \n",
    "    ari_score = adjusted_rand_score(kmeans_labels_ordered, graph_labels_ordered)\n",
    "    nmi_score = normalized_mutual_info_score(kmeans_labels_ordered, graph_labels_ordered)\n",
    "    print(f\"\\nAdjusted Rand Index (ARI) between K-Means and Louvain: {ari_score:.3f}\")\n",
    "    print(f\"Normalized Mutual Information (NMI) between K-Means and Louvain: {nmi_score:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo common nodes for comparison. Something might have gone wrong with graph construction or clustering.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(G.to_undirected()))\n",
    "print(f\"Number of connected components: {len(components)}\")\n",
    "\n",
    "for i, comp in enumerate(components):\n",
    "    print(f\"Component {i}: {len(comp)} nodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ded434",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "### Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ffda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undirected = G.to_undirected()\n",
    "\n",
    "communities = list(greedy_modularity_communities(G_undirected))\n",
    "\n",
    "mod_score = modularity(G_undirected, communities)\n",
    "\n",
    "print(f\"Modularity score: {mod_score:.4f}\")\n",
    "print(f\"Detected Communities: {len(communities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ad222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "top_pagerank = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top_pagerank:\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6410b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hits_hubs, hits_authorities = nx.hits(G_largest, max_iter=1000)\n",
    "\n",
    "print(\"Authority Nodes\")\n",
    "for node, score in sorted(hits_authorities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{node}: {score:.4f}\")\n",
    "    \n",
    "print(\"Hub Nodes\")\n",
    "for node, score in sorted(hits_hubs.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb8877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcc = max(nx.weakly_connected_components(G_largest), key=len)\n",
    "G_wcc = G_largest.subgraph(wcc).copy()\n",
    "\n",
    "avg_path_len = nx.average_shortest_path_length(G_wcc.to_undirected())\n",
    "diameter = nx.diameter(G_largest.to_undirected())\n",
    "density = nx.density(G_largest)\n",
    "\n",
    "print(f\" Average Path Length: {avg_path_len:.4f}\")\n",
    "print(f\" Diameter {diameter}\")\n",
    "print(f\" Density: {density:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weak_components = nx.number_weakly_connected_components(G)\n",
    "num_strong_components = nx.number_strongly_connected_components(G)\n",
    "\n",
    "print(f\"Weakly connected components: {num_weak_components}\")\n",
    "print(f\"Strongly connected components: {num_strong_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e622dab",
   "metadata": {},
   "source": [
    "## Hits@N / MRR\n",
    "\n",
    "### Extracting links between articles in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_titles(title):\n",
    "    return title.strip().replace(\" \", \"_\").capitalize()\n",
    "\n",
    "def get_backlinks(title, all_titles_set, sleep_time=0.5):\n",
    "    backlinks = set()\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    formatted_title = format_titles(title)\n",
    "    params = {\n",
    "        \"action\" : \"query\",\n",
    "        \"list\"   : \"backlinks\",\n",
    "        \"bltitle\": formatted_title,\n",
    "        \"bllimit\": \"max\",\n",
    "        \"format\" : \"json\"\n",
    "    }    \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params).json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {title}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if 'query' in response:\n",
    "            links = [entry['title'] for entry in response['query']['backlinks']]\n",
    "            filtered = [link for link in links if link in all_titles_set]\n",
    "            backlinks.update(filtered)\n",
    "        \n",
    "        if 'continue' in response:\n",
    "            params.update(response['continue'])\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return backlinks if backlinks else set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Saving backlings\n",
    "#Takes a while, onlty run once\n",
    "\n",
    "#all_titles_set = set(df['title'].str.strip())\n",
    "#\n",
    "#ground_truth_links = {}\n",
    "#\n",
    "#for title in df['title']:\n",
    "#    incoming = get_backlinks(title, all_titles_set)\n",
    "#    ground_truth_links[title] = incoming\n",
    "#    \n",
    "#with open('ground_truth_links.pkl', 'wb') as f:\n",
    "#    pickle.dump(ground_truth_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ground_truth_links.pkl', 'rb') as f:\n",
    "    ground_truth_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40210dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_backlinks = sum(len(links) for links in ground_truth_links.values() if links is not None)\n",
    "print(f\"Total Backlinks: {total_backlinks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d272678",
   "metadata": {},
   "source": [
    "### Hits@N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_incoming = defaultdict(list)\n",
    "\n",
    "for src, tgt, key, data in G.edges(data=True, keys=True):\n",
    "    weight = data.get('weight', 0.0)\n",
    "    subtopic_pair = data.get('subtopic_pair')\n",
    "    predicted_incoming[tgt].append((src,weight,subtopic_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hits_at_n(ground_truth, predicted_incoming, N=10):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    \n",
    "    for tgt_doc, true_sources in ground_truth.items():\n",
    "        if not true_sources:\n",
    "            continue\n",
    "            \n",
    "        preds = predicted_incoming.get(tgt_doc, [])\n",
    "        \n",
    "        # Keep only best edge per unique source\n",
    "        best_edges = {}\n",
    "        for src, weight, _ in preds:\n",
    "            if src not in best_edges or weight > best_edges[src]:\n",
    "                best_edges[src] = weight\n",
    "        \n",
    "        top_n = sorted(best_edges.items(), key=lambda x: x[1], reverse=True)[:N]        \n",
    "        top_n_preds = [src for src, _ in top_n]\n",
    "        \n",
    "        if any(p in true_sources for p in top_n_preds):\n",
    "            hits += 1\n",
    "        total += 1\n",
    "        \n",
    "    return hits / total if total else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8300802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(ground_truth, predicted_incoming):\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for tgt_doc, true_sources in ground_truth.items():\n",
    "        if not true_sources:\n",
    "            continue\n",
    "            \n",
    "        preds = predicted_incoming.get(tgt_doc, [])\n",
    "        \n",
    "        best_edges = {}\n",
    "        for src, weight, _ in preds:\n",
    "            if src not in best_edges or weight > best_edges[src]:\n",
    "                best_edges[src] = weight        \n",
    "        \n",
    "        ranked_sources = sorted(best_edges.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranked_src_ids = [src for src, _ in ranked_sources]\n",
    "        \n",
    "        for rank, src in enumerate(ranked_src_ids, start=1):\n",
    "            if src in true_sources:\n",
    "                reciprocal_ranks.append(1/rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "    \n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits1  = compute_hits_at_n(ground_truth_links, predicted_incoming, N=1)\n",
    "hits3  = compute_hits_at_n(ground_truth_links, predicted_incoming, N=3)\n",
    "hits5  = compute_hits_at_n(ground_truth_links, predicted_incoming, N=5)\n",
    "hits10 = compute_hits_at_n(ground_truth_links, predicted_incoming, N=10)\n",
    "mrr    = compute_mrr(ground_truth_links, predicted_incoming)\n",
    "\n",
    "print(f\"Hits@1: {hits1:.4f}\")\n",
    "print(f\"Hits@3: {hits3:.4f}\")\n",
    "print(f\"Hits@5: {hits5:.4f}\")\n",
    "print(f\"Hits@10: {hits10:.4f}\")\n",
    "print(f\"MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991bf9b",
   "metadata": {},
   "source": [
    "# Link Induction\n",
    "\n",
    "## PyTorch Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Mapping or node name to integer index\n",
    "node_to_idx = {node: i for i, node in enumerate(G.nodes())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load UMAP reduced TF-IDF embeddings:\n",
    "embedding = np.load(\"umap_embeddings.npy\")\n",
    "print(\"UMAP embedding shape:\", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854183df",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_titles = df['title'].values\n",
    "title_to_idx ={title: i for i, title in enumerate(doc_titles)}\n",
    "\n",
    "#One-hot encoded cluster labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cluster_onehot = encoder.fit_transform(df[['cluster']])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embedding = scaler.fit_transform(embedding)\n",
    "\n",
    "full_features = np.hstack([scaled_embedding, cluster_onehot])\n",
    "\n",
    "# Node features for PyG graph \n",
    "node_features = []\n",
    "for node in G.nodes():\n",
    "    idx = title_to_idx.get(node)\n",
    "    if idx is not None:\n",
    "        vec = full_features[idx]\n",
    "        node_features.append(vec)\n",
    "    else:\n",
    "        node_features.append(np.zeros(tfidf_matrix.shape[1]))\n",
    "\n",
    "node_features_array = np.array(node_features)\n",
    "x = torch.tensor(node_features_array, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_sources   = []\n",
    "edge_targets   = []\n",
    "edge_weights   = []\n",
    "edge_sims      = []\n",
    "edge_subtopics = []\n",
    "\n",
    "for src_title, tgt_title, key, attr in G.edges(data=True, keys=True):\n",
    "    src_idx = title_to_idx.get(src_title)\n",
    "    tgt_idx = title_to_idx.get(tgt_title)\n",
    "    \n",
    "    #Skip if node is missing\n",
    "    if src_idx is None or tgt_idx is None:\n",
    "        continue\n",
    "    \n",
    "    edge_sources.append(src_idx)\n",
    "    edge_targets.append(tgt_idx)\n",
    "    \n",
    "    # get edge attributes\n",
    "    edge_weights.append(attr.get('weight',1.0))\n",
    "    edge_sims.append(attr.get('sim',0.0))\n",
    "    edge_subtopics.append(attr.get('subtopic_pair', ('NA', 'NA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
    "edge_sim = torch.tensor(edge_sims, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975afbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of all subtopic labels from topic_labels\n",
    "all_subtopics = set()\n",
    "\n",
    "# Add subtopics  to vocab\n",
    "for topics in df['topic_list']:\n",
    "    all_subtopics.update(topics)\n",
    "    \n",
    "subtopic_to_idx = {sub: i for i, sub in enumerate(sorted(all_subtopics))}\n",
    "\n",
    "# Convert subtopic pairs to pairs of indices\n",
    "edge_subtopic_idx = torch.tensor(\n",
    "    [[subtopic_to_idx[s1], subtopic_to_idx[s2]] for s1, s2 in edge_subtopics],\n",
    "    dtype=torch.long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    edge_weight=edge_weight,\n",
    "    edge_sim=edge_sim,\n",
    "    edge_subtopic=edge_subtopic_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Graph\n",
    "#torch.save(data,\"multidigraph.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d0690",
   "metadata": {},
   "source": [
    "## GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges = data.edge_subtopic.shape[0]\n",
    "data.edge_subtopic.shape == (num_edges,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c91215",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_stratified_link_split_nodes_disjointed(\n",
    "    G, df, title_to_idx, similarity_df, subtopic_to_idx, subtopic_edge_dict,\n",
    "    hard_negative_sim_threshold=0.5,\n",
    "    test_size=0.2, random_state=42\n",
    "):\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    doc_titles_list = df['title'].tolist()\n",
    "    all_global_node_ids = list(range(len(doc_titles_list)))\n",
    "    \n",
    "    node_connectivity_status = {}\n",
    "    for node_id in all_global_node_ids:\n",
    "        node_title = doc_titles_list[node_id]\n",
    "        if G.degree(node_title) == 0:\n",
    "            node_connectivity_status[node_id] = 'isolated'\n",
    "        else:\n",
    "            node_connectivity_status[node_id] = 'connected'\n",
    "    \n",
    "    isolated_count = sum(1 for status in node_connectivity_status.values() if status == 'isolated')\n",
    "    connected_count = len(all_global_node_ids) - isolated_count\n",
    "    print(f\"Total Nodes: {len(all_global_node_ids)}\")\n",
    "    print(f\"Isolated Nodes: {isolated_count}\")\n",
    "    print(f\"Connected Nodes: {connected_count}\")\n",
    "    print(f\"Ratio Isolated:Connected = {isolated_count / connected_count:.2f}\" if connected_count > 0 else \"N/A\")\n",
    "    print(f\"--- End Node Connectivity Summary ---\")\n",
    "        \n",
    "    raw_stratify_keys = [\n",
    "        f\"cluster_{df.loc[df['title'] == doc_titles_list[nid], 'cluster'].values[0]}_\"\n",
    "        f\"{node_connectivity_status[nid]}\"\n",
    "        for nid in all_global_node_ids\n",
    "    ]\n",
    "    \n",
    "    from collections import Counter\n",
    "    key_counts = Counter(raw_stratify_keys)\n",
    "    \n",
    "    min_members_for_stratify = 3\n",
    "    final_stratify_keys = []\n",
    "    for key in raw_stratify_keys:\n",
    "        if key_counts[key] < min_members_for_stratify:\n",
    "            final_stratify_keys.append('mis_stratify_category')\n",
    "        else:\n",
    "            final_stratify_keys.append(key)\n",
    "    \n",
    "    train_node_ids, test_node_ids = train_test_split(\n",
    "        all_global_node_ids,\n",
    "        test_size=test_size,\n",
    "        stratify=final_stratify_keys,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    train_node_ids_set = set(train_node_ids)\n",
    "    test_node_ids_set = set(test_node_ids)\n",
    "    \n",
    "    # Check for Node Overlap\n",
    "    node_overlap_count = len(train_node_ids_set.intersection(test_node_ids_set))\n",
    "    print(f\"\\n--- Node Overlap Check ---\")\n",
    "    print(f\"Nodes in Training Set: {len(train_node_ids_set)}\")\n",
    "    print(f\"Nodes in Test Set: {len(test_node_ids_set)}\")\n",
    "    print(f\"Overlap (Train Nodes & Test Nodes): {node_overlap_count} (Expected: 0)\")\n",
    "    assert node_overlap_count == 0, \"Node sets overlap! Split is not node-disjoint.\"\n",
    "    print(f\"--- Node Overlap Check Complete ---\")\n",
    "    \n",
    "    train_edges = []\n",
    "    test_edges = []\n",
    "    all_pos_edges_in_G = []\n",
    "    \n",
    "    for src_title, tgt_title, data in G.edges(data=True):\n",
    "        src_id = title_to_idx.get(src_title)\n",
    "        tgt_id = title_to_idx.get(tgt_title)\n",
    "        if src_id is None or tgt_id is None: continue\n",
    "\n",
    "        s1_name, s2_name = data.get('subtopic_pair', (None, None))\n",
    "        s_idx1 = subtopic_to_idx.get(s1_name, 0)\n",
    "        s_idx2 = subtopic_to_idx.get(s2_name, 0)\n",
    "        \n",
    "        edge_data_dict = {\n",
    "            'src': src_id, 'tgt': tgt_id, 'subtopic' : [s_idx1, s_idx2],\n",
    "            'weight' : data.get('weight', 0.0), 'sim' : data.get('sim', 0.0),\n",
    "            'cluster': (df.loc[df['title'] == src_title, 'cluster'].values[0], df.loc[df['title'] == tgt_title, 'cluster'].values[0])\n",
    "        }\n",
    "        all_pos_edges_in_G.append(edge_data_dict)\n",
    "        \n",
    "        if src_id in train_node_ids_set and tgt_id in train_node_ids_set:\n",
    "            train_edges.append(edge_data_dict)\n",
    "        elif src_id in test_node_ids_set and tgt_id in test_node_ids_set:\n",
    "            test_edges.append(edge_data_dict)\n",
    "\n",
    "    def extract_tensor_dict(edges_list_of_dicts):\n",
    "        if not edges_list_of_dicts:\n",
    "            return {\n",
    "                'edge_index': torch.empty((2, 0), dtype=torch.long), 'subtopic' : torch.empty((0, 2), dtype=torch.long),\n",
    "                'weight': torch.empty(0, dtype=torch.float), 'sim': torch.empty(0, dtype=torch.float),\n",
    "            }\n",
    "        return {\n",
    "            'edge_index': torch.tensor([[e['src'], e['tgt']] for e in edges_list_of_dicts], dtype=torch.long).T,\n",
    "            'subtopic'  : torch.tensor([e['subtopic'] for e in edges_list_of_dicts], dtype=torch.long),\n",
    "            'weight'    : torch.tensor([e['weight'] for e in edges_list_of_dicts], dtype=torch.float),\n",
    "            'sim'       : torch.tensor([e['sim'] for e in edges_list_of_dicts], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "    edge_group_dict = defaultdict(list)\n",
    "    cluster_map = {}\n",
    "    for edge_data_processed in all_pos_edges_in_G:\n",
    "        pair = (edge_data_processed['src'], edge_data_processed['tgt'])\n",
    "        edge_group_dict[pair].append(edge_data_processed)\n",
    "        if pair not in cluster_map:\n",
    "            src_id_for_cluster = edge_data_processed['src']\n",
    "            src_title_for_cluster = doc_titles_list[src_id_for_cluster]\n",
    "            src_cluster = df.loc[df['title'] == src_title_for_cluster, 'cluster'].values[0]\n",
    "            cluster_map[pair] = src_cluster\n",
    "        \n",
    "    pairs_to_split = list(edge_group_dict.keys())\n",
    "    clusters_for_stratify = [cluster_map[pair] for pair in pairs_to_split]\n",
    "    \n",
    "    train_pairs, test_pairs = train_test_split(\n",
    "        pairs_to_split, test_size=test_size, stratify=clusters_for_stratify, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Convert all existing positive edge pairs to a symmetric set\n",
    "    all_pos_edge_pairs_set_global = set((e['src'], e['tgt']) for e in all_pos_edges_in_G)\n",
    "    reversed_pos_edges_to_add = [(tgt, src) for src, tgt in list(all_pos_edge_pairs_set_global)]\n",
    "    all_pos_edge_pairs_set_global.update(reversed_pos_edges_to_add)\n",
    "\n",
    "    # Define all TRAIN POSITIVE edge pairs\n",
    "    train_pos_edge_ids_set_sym = set(pair for pair in train_pairs)\n",
    "    reversed_train_pos_to_add = [(tgt, src) for src, tgt in list(train_pos_edge_ids_set_sym)]\n",
    "    train_pos_edge_ids_set_sym.update(reversed_train_pos_to_add)\n",
    "    \n",
    "    # Define all TEST POSITIVE edge pairs\n",
    "    test_pos_edge_ids_set_sym = set(pair for pair in test_pairs)\n",
    "    reversed_test_pos_to_add = [(tgt, src) for src, tgt in list(test_pos_edge_ids_set_sym)]\n",
    "    test_pos_edge_ids_set_sym.update(reversed_test_pos_to_add)\n",
    "\n",
    "    # Samples from the full graph node IDs and avoids specific sets of edges.\n",
    "    def generate_negatives(num_samples_needed, hard_type, source_node_pool, target_node_pool, current_avoid_set): \n",
    "        negatives_list_of_dicts = []\n",
    "        attempts_count = 0\n",
    "        max_attempts_for_gen = num_samples_needed * 100 \n",
    "        \n",
    "        source_node_pool_list = list(source_node_pool)\n",
    "        target_node_pool_list = list(target_node_pool)\n",
    "\n",
    "        while len(negatives_list_of_dicts) < num_samples_needed and attempts_count < max_attempts_for_gen:\n",
    "            i = random.choice(source_node_pool_list)\n",
    "            j = random.choice(target_node_pool_list)\n",
    "            \n",
    "            if i == j: # Skip self-loops\n",
    "                attempts_count +=1\n",
    "                continue\n",
    "\n",
    "            if (i,j) in current_avoid_set: \n",
    "                attempts_count +=1\n",
    "                continue\n",
    "            if (j,i) in current_avoid_set:\n",
    "                attempts_count +=1\n",
    "                continue\n",
    "                \n",
    "            subs_i = df.loc[df['title'] == doc_titles_list[i], 'topic_list'].values[0]\n",
    "            subs_j = df.loc[df['title'] == doc_titles_list[j], 'topic_list'].values[0]\n",
    "            \n",
    "            best_sim = -1.0\n",
    "            best_pair_names = (None, None)\n",
    "            if subs_i and subs_j:\n",
    "                for s1_name in subs_i:\n",
    "                    for s2_name in subs_j:\n",
    "                        sim_val_from_df = similarity_df.get(s1_name, {}).get(s2_name)\n",
    "                        if sim_val_from_df is not None:\n",
    "                            if sim_val_from_df > best_sim:\n",
    "                                best_sim = sim_val_from_df\n",
    "                                best_pair_names = (s1_name, s2_name)\n",
    "            \n",
    "            if best_sim == -1.0: \n",
    "                attempts_count += 1\n",
    "                continue\n",
    "            if (hard_type and best_sim < hard_negative_sim_threshold) or \\\n",
    "               (not hard_type and best_sim >= hard_negative_sim_threshold/2): continue\n",
    "            \n",
    "            weight = subtopic_edge_dict.get(best_pair_names, 0.0)\n",
    "\n",
    "            negatives_list_of_dicts.append({\n",
    "                'src': i, 'tgt': j,\n",
    "                'subtopic': [subtopic_to_idx.get(best_pair_names[0], 0), subtopic_to_idx.get(best_pair_names[1], 0)],\n",
    "                'weight': weight, 'sim': best_sim\n",
    "            })\n",
    "            current_avoid_set.add((i,j)) \n",
    "            current_avoid_set.add((j,i)) \n",
    "            attempts_count += 1 \n",
    "\n",
    "        print(f\"Generated {len(negatives_list_of_dicts)} negatives for is_hard={hard_type} after {attempts_count} attempts.\")\n",
    "        return negatives_list_of_dicts\n",
    "    \n",
    "    # Generate TRAIN Easy Negatives\n",
    "    current_global_avoid_set = all_pos_edge_pairs_set_global.copy() # Start with all existing graph edges\n",
    "    \n",
    "    train_neg_edges_easy_list = generate_negatives(len(train_edges), hard_type=False, \n",
    "                                                   source_node_pool=train_node_ids_set, \n",
    "                                                   target_node_pool=train_node_ids_set,\n",
    "                                                   current_avoid_set=current_global_avoid_set)\n",
    "    \n",
    "    # Generate TRAIN Hard Negatives\n",
    "    train_neg_edges_hard_list = generate_negatives(len(train_edges), hard_type=True,\n",
    "                                                   source_node_pool=train_node_ids_set, \n",
    "                                                   target_node_pool=train_node_ids_set,\n",
    "                                                   current_avoid_set=current_global_avoid_set)\n",
    "\n",
    "    # Generate TEST Easy Negatives\n",
    "    test_neg_edges_easy_list = generate_negatives(len(test_edges), hard_type=False,\n",
    "                                                  source_node_pool=train_node_ids_set, \n",
    "                                                  target_node_pool=train_node_ids_set,\n",
    "                                                  current_avoid_set=current_global_avoid_set)\n",
    "\n",
    "    # Generate TEST Hard Negatives\n",
    "    test_neg_edges_hard_list = generate_negatives(len(test_edges), hard_type=True,\n",
    "                                                  source_node_pool=train_node_ids_set, \n",
    "                                                  target_node_pool=train_node_ids_set,\n",
    "                                                  current_avoid_set=current_global_avoid_set)\n",
    "\n",
    "    return {\n",
    "        'train_pos': extract_tensor_dict(train_edges),\n",
    "        'test_pos': extract_tensor_dict(test_edges),\n",
    "        'train_neg_easy': extract_tensor_dict(train_neg_edges_easy_list),\n",
    "        'train_neg_hard': extract_tensor_dict(train_neg_edges_hard_list),\n",
    "        'test_neg_easy' : extract_tensor_dict(test_neg_edges_easy_list),\n",
    "        'test_neg_hard' : extract_tensor_dict(test_neg_edges_hard_list)\n",
    "    }, train_node_ids_set, test_node_ids_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4705f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, train_node_ids_set, test_node_ids_set = create_stratified_link_split_nodes_disjointed(\n",
    "    G=G,\n",
    "    df=df,\n",
    "    similarity_df=similarity_df,\n",
    "    subtopic_to_idx=subtopic_to_idx,\n",
    "    subtopic_edge_dict=subtopic_edge_dict,\n",
    "    title_to_idx=title_to_idx,\n",
    "    test_size=0.2,\n",
    "    hard_negative_sim_threshold=0.45,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits\n",
    "train_pos_set_check = set(tuple(e) for e in splits['train_pos']['edge_index'].T.cpu().tolist())\n",
    "train_pos_set_check.update(set((tgt, src) for src, tgt in train_pos_set_check))\n",
    "\n",
    "# test_pos\n",
    "test_pos_set_check = set(tuple(e) for e in splits['test_pos']['edge_index'].T.cpu().tolist())\n",
    "test_pos_set_check.update(set((tgt, src) for src, tgt in test_pos_set_check))\n",
    "\n",
    "# train_neg_easy\n",
    "train_neg_easy_set_check = set(tuple(e) for e in splits['train_neg_easy']['edge_index'].T.cpu().tolist())\n",
    "train_neg_easy_set_check.update(set((tgt, src) for src, tgt in train_neg_easy_set_check))\n",
    "\n",
    "# train_neg_hard\n",
    "train_neg_hard_set_check = set(tuple(e) for e in splits['train_neg_hard']['edge_index'].T.cpu().tolist())\n",
    "train_neg_hard_set_check.update(set((tgt, src) for src, tgt in train_neg_hard_set_check))\n",
    "\n",
    "# test_neg_easy\n",
    "test_neg_easy_set_check = set(tuple(e) for e in splits['test_neg_easy']['edge_index'].T.cpu().tolist())\n",
    "test_neg_easy_set_check.update(set((tgt, src) for src, tgt in test_neg_easy_set_check))\n",
    "\n",
    "# test_neg_hard\n",
    "test_neg_hard_set_check = set(tuple(e) for e in splits['test_neg_hard']['edge_index'].T.cpu().tolist())\n",
    "test_neg_hard_set_check.update(set((tgt, src) for src, tgt in test_neg_hard_set_check))\n",
    "\n",
    "\n",
    "print(\"\\n Positive Edge Leakage\")\n",
    "overlap_train_pos_test_pos = len(train_pos_set_check.intersection(test_pos_set_check))\n",
    "print(f\"Train Pos vs Test Pos overlap: {overlap_train_pos_test_pos}\")\n",
    "\n",
    "print(\"\\nNegative Edge Leakage\")\n",
    "overlap_train_neg_easy_test_neg_easy = len(train_neg_easy_set_check.intersection(test_neg_easy_set_check))\n",
    "print(f\"Train Neg Easy vs Test Neg Easy overlap: {overlap_train_neg_easy_test_neg_easy}\")\n",
    "\n",
    "overlap_train_neg_hard_test_neg_hard = len(train_neg_hard_set_check.intersection(test_neg_hard_set_check))\n",
    "print(f\"Train Neg Hard vs Test Neg Hard overlap: {overlap_train_neg_hard_test_neg_hard}\")\n",
    "\n",
    "print(\"\\nCross-Split Leakage (e.g., test in training negs, or vice-versa\")\n",
    "# Test positives in any training negative set\n",
    "overlap_test_pos_train_neg_easy = len(test_pos_set_check.intersection(train_neg_easy_set_check))\n",
    "print(f\"Test Pos vs Train Neg Easy overlap: {overlap_test_pos_train_neg_easy}\")\n",
    "\n",
    "overlap_test_pos_train_neg_hard = len(test_pos_set_check.intersection(train_neg_hard_set_check))\n",
    "print(f\"Test Pos vs Train Neg Hard overlap: {overlap_test_pos_train_neg_hard}\")\n",
    "\n",
    "# Test negatives in any training positive set\n",
    "overlap_test_neg_easy_train_pos = len(test_neg_easy_set_check.intersection(train_pos_set_check))\n",
    "print(f\"Test Neg Easy vs Train Pos overlap: {overlap_test_neg_easy_train_pos}\")\n",
    "\n",
    "overlap_test_neg_hard_train_pos = len(test_neg_hard_set_check.intersection(train_pos_set_check))\n",
    "print(f\"Test Neg Hard vs Train Pos overlap: {overlap_test_neg_hard_train_pos}\")\n",
    "\n",
    "# Check for any test negative in any train negative\n",
    "all_train_neg_set_check = train_neg_easy_set_check.union(train_neg_hard_set_check)\n",
    "all_test_neg_set_check = test_neg_easy_set_check.union(test_neg_hard_set_check)\n",
    "overlap_all_train_neg_all_test_neg = len(all_train_neg_set_check.intersection(all_test_neg_set_check))\n",
    "print(f\"All Train Neg vs All Test Neg overlap: {overlap_all_train_neg_all_test_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83177def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "print(\"\\n--- Verifying Node-Disjoint Negative Edge Integrity ---\")\n",
    "\n",
    "# Get all training negative edges\n",
    "all_train_neg_edges = []\n",
    "all_train_neg_edges.extend(splits['train_neg_easy']['edge_index'].T.cpu().tolist())\n",
    "all_train_neg_edges.extend(splits['train_neg_hard']['edge_index'].T.cpu().tolist())\n",
    "\n",
    "# Get all test negative edges \n",
    "all_test_neg_edges = []\n",
    "all_test_neg_edges.extend(splits['test_neg_easy']['edge_index'].T.cpu().tolist())\n",
    "all_test_neg_edges.extend(splits['test_neg_hard']['edge_index'].T.cpu().tolist())\n",
    "\n",
    "# Define node sets for checking\n",
    "train_nodes = train_node_ids_set \n",
    "test_nodes = test_node_ids_set   \n",
    "\n",
    "# Function to check for cross-split edges\n",
    "def check_cross_split_edges(edge_list, source_set, target_set, description):\n",
    "    cross_split_count = 0\n",
    "    for src, tgt in edge_list:\n",
    "        is_src_in_source = src in source_set\n",
    "        is_tgt_in_target = tgt in target_set\n",
    "        \n",
    "        if is_src_in_source and is_tgt_in_target:\n",
    "            pass\n",
    "        elif (is_src_in_source and tgt in test_nodes) or \\\n",
    "             (is_src_in_source and tgt in train_nodes) or \\\n",
    "             (is_tgt_in_target and src in test_nodes) or \\\n",
    "             (is_tgt_in_target and src in train_nodes):\n",
    "             pass \n",
    "\n",
    "\n",
    "    cross_split_edges_found = []\n",
    "    for src, tgt in edge_list:\n",
    "        src_is_train = src in train_nodes\n",
    "        tgt_is_train = tgt in train_nodes\n",
    "        src_is_test = src in test_nodes\n",
    "        tgt_is_test = tgt in test_nodes\n",
    "\n",
    "        if (src_is_train and tgt_is_test) or (src_is_test and tgt_is_train):\n",
    "            cross_split_edges_found.append((src, tgt))\n",
    "    \n",
    "    print(f\"{description}: Found {len(cross_split_edges_found)} cross-split edges (Expected: 0)\")\n",
    "    if len(cross_split_edges_found) > 0:\n",
    "        print(f\"  Sample cross-split edges: {cross_split_edges_found[:5]}\")\n",
    "\n",
    "\n",
    "# Check Training Negatives\n",
    "print(\"\\n--- Checking Training Negative Edges ---\")\n",
    "check_cross_split_edges(all_train_neg_edges, train_nodes, test_nodes, \"All Train Negatives\")\n",
    "\n",
    "# Check Test Negatives\n",
    "print(\"\\n--- Checking Test Negative Edges ---\")\n",
    "check_cross_split_edges(all_test_neg_edges, train_nodes, test_nodes, \"All Test Negatives\")\n",
    "\n",
    "print(\"\\n--- Node-Disjoint Negative Edge Integrity Check Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032db0e",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter=0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stop=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_gat_model(\n",
    "    model,\n",
    "    data,\n",
    "    splits,\n",
    "    num_epochs=1000,\n",
    "    hard_neg_start_epoch=30,\n",
    "    patience=10,\n",
    "    min_epochs=40,\n",
    "    lr=0.001,\n",
    "    neg_cycle_length_epochs=10,\n",
    "    device='cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    early_stopper = EarlyStopping(patience)\n",
    "    checkpoint_path = 'best_model.pt'\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Unpack Splits\n",
    "    train_pos      = splits['train_pos']\n",
    "    test_pos       = splits['test_pos']\n",
    "    train_neg_easy = splits['train_neg_easy']\n",
    "    train_neg_hard = splits['train_neg_hard']\n",
    "    test_neg_easy  = splits['test_neg_easy']\n",
    "    test_neg_hard  = splits['test_neg_hard']\n",
    "\n",
    "    combined_train_negs = {\n",
    "        'edge_index': torch.cat([train_neg_easy['edge_index'], train_neg_hard['edge_index']], dim=1),\n",
    "        'subtopic': torch.cat([train_neg_easy['subtopic'], train_neg_hard['subtopic']], dim=0),\n",
    "        'weight': torch.cat([train_neg_easy['weight'], train_neg_hard['weight']], dim=0),\n",
    "        'sim': torch.cat([train_neg_easy['sim'], train_neg_hard['sim']], dim=0)\n",
    "    }\n",
    "\n",
    "    # Tracking\n",
    "    history = {\n",
    "        'train_loss'    : [],\n",
    "        'val_easy_loss' : [],\n",
    "        'val_easy_auc'  : [],\n",
    "        'val_hard_loss' : [],\n",
    "        'val_hard_auc'  : []\n",
    "    }\n",
    "\n",
    "    # Combine positive & Negative edges into one dataset per epoch\n",
    "    def prepare_batch(pos, neg):\n",
    "        edge_index = torch.cat([pos['edge_index'], neg['edge_index']], dim=1).to(device)\n",
    "        edge_subtopic = torch.cat([pos['subtopic'], neg['subtopic']], dim=0).to(device)\n",
    "        edge_weight = torch.cat([pos['weight'],neg['weight']], dim=0).to(device)\n",
    "        edge_sim = torch.cat([pos['sim'], neg['sim']], dim=0).to(device)\n",
    "\n",
    "        labels = torch.cat([\n",
    "            torch.ones(pos['edge_index'].shape[1], dtype=torch.float),\n",
    "            torch.zeros(neg['edge_index'].shape[1], dtype=torch.float)\n",
    "        ]).to(device)\n",
    "\n",
    "        return edge_index, edge_subtopic, edge_weight, edge_sim, labels\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Adding Curriculum (Swithcing to hard negatives after N epochs)\n",
    "        if epoch < hard_neg_start_epoch:\n",
    "            neg_train = train_neg_easy\n",
    "        else:\n",
    "            cycle_number = (epoch - hard_neg_start_epoch) // neg_cycle_length_epochs\n",
    "\n",
    "            if cycle_number % 2 == 0:\n",
    "                neg_train = train_neg_easy\n",
    "            else:\n",
    "                neg_train = train_neg_hard\n",
    "\n",
    "        edge_index, subtopic, weight, sim, labels = prepare_batch(train_pos, neg_train)\n",
    "        node_feats = data.x.to(device)\n",
    "\n",
    "        preds = model(node_feats, edge_index, subtopic, weight, sim)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_results = {}\n",
    "            for neg_type, neg_split in {'Easy': test_neg_easy, 'Hard': test_neg_hard}.items():\n",
    "                val_edge_index, val_subtopic, val_weight, val_sim, val_labels = prepare_batch(test_pos, neg_split)\n",
    "                val_preds = model(node_feats, val_edge_index, val_subtopic, val_weight, val_sim)\n",
    "                val_loss = loss_fn(val_preds, val_labels).item()\n",
    "                val_auc = roc_auc_score(val_labels.cpu(), val_preds.cpu())\n",
    "\n",
    "                val_probs = torch.sigmoid(val_preds).cpu().numpy()\n",
    "                val_bin_preds = (val_probs >= 0.5).astype(int)\n",
    "                val_labels_np = val_labels.cpu().numpy()\n",
    "\n",
    "                precision =  precision_score(val_labels_np, val_bin_preds)\n",
    "                recall = recall_score(val_labels_np, val_bin_preds)\n",
    "                mrr = average_precision_score(val_labels_np, val_probs)\n",
    "\n",
    "                val_results[neg_type] = {\n",
    "                    'loss' : val_loss,\n",
    "                    'auc' : val_auc,\n",
    "                    'precision' : precision,\n",
    "                    'recall' : recall,\n",
    "                    'mrr' : mrr\n",
    "                }\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"Train Loss       : {loss.item():.4f}\")\n",
    "\n",
    "        print(\"\\n[Validation - Easy Negatives]\")\n",
    "        print(f\"  Loss         : {val_results['Easy']['loss']:.4f}\")\n",
    "        print(f\"  AUC          : {val_results['Easy']['auc']:.4f}\")\n",
    "        print(f\"  Precision    : {val_results['Easy']['precision']:.4f}\")\n",
    "        print(f\"  Recall       : {val_results['Easy']['recall']:.4f}\")\n",
    "        print(f\"  MRR          : {val_results['Easy']['mrr']:.4f}\")\n",
    "\n",
    "        print(\"\\n[Validation - Hard Negatives]\")\n",
    "        print(f\"  Loss         : {val_results['Hard']['loss']:.4f}\")\n",
    "        print(f\"  AUC          : {val_results['Hard']['auc']:.4f}\")\n",
    "        print(f\"  Precision    : {val_results['Hard']['precision']:.4f}\")\n",
    "        print(f\"  Recall       : {val_results['Hard']['recall']:.4f}\")\n",
    "        print(f\"  MRR          : {val_results['Hard']['mrr']:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "        # Save Best Model\n",
    "        if val_results['Hard']['loss'] < best_val_loss:\n",
    "            best_val_loss = val_results['Hard']['loss']\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        #Update History\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_easy_loss'].append(val_results['Easy']['loss'])\n",
    "        history['val_easy_auc'].append(val_results['Easy']['auc'])\n",
    "        history['val_hard_loss'].append(val_results['Hard']['loss'])\n",
    "        history['val_hard_auc'].append(val_results['Hard']['auc'])\n",
    "\n",
    "        # Early stopping based on validation with hard negatives\n",
    "        if epoch >= min_epochs:\n",
    "            early_stopper(val_results['Hard']['loss'])\n",
    "            if early_stopper.stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    #Plot Training Curve\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history['train_loss'], label='Train loss')\n",
    "    plt.plot(history['val_easy_loss'], label='Val Easy Loss')\n",
    "    plt.plot(history['val_hard_loss'], label = 'Val Hard Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history['val_easy_auc'], label='Val Easy AUC')\n",
    "    plt.plot(history['val_hard_auc'], label='Val Hard AUC')\n",
    "    plt.title('Validation AUC')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('AUC'); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class SubtopicGAT(torch.nn.Module):\n",
    "    # Parameters = default values\n",
    "    def __init__(self, in_channels, hidden_channels, num_subtopics, edge_emb_dim=16, heads=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Subtopic pair embedding\n",
    "        self.subtopic_embed = nn.Embedding(num_subtopics,edge_emb_dim)\n",
    "\n",
    "        # GAT Layer\n",
    "        self.gat1 = GATConv(in_channels,hidden_channels,heads=heads, concat=True, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout)\n",
    "\n",
    "        #Decoder MLP for edge scoring\n",
    "        self.edge_decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels + 2 * edge_emb_dim + 2, 64), #+2 for edge_weight and edge_sim\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_subtopic, edge_weight, edge_sim):\n",
    "        # Encode Node Features\n",
    "        x = self.gat1(x, edge_index).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "\n",
    "        # Compute edge-wise node embeddings\n",
    "        src, tgt = edge_index\n",
    "        h_src = x[src]\n",
    "        h_tgt = x[tgt]\n",
    "\n",
    "        # Subtopic Embeddings\n",
    "        s1 = self.subtopic_embed(edge_subtopic[:,0])\n",
    "        s2 = self.subtopic_embed(edge_subtopic[:,1])\n",
    "        edge_subtopic_feat = torch.cat([s1,s2],dim=1)\n",
    "\n",
    "        # Combine edge features\n",
    "        edge_feats = torch.cat([\n",
    "            h_src, h_tgt,\n",
    "            edge_subtopic_feat,\n",
    "            edge_weight.unsqueeze(1),\n",
    "            edge_sim.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "\n",
    "        return self.edge_decoder(edge_feats).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = data.x.shape[1] \n",
    "hidden_channels = 64         \n",
    "num_subtopics   = len(subtopic_to_idx)\n",
    "\n",
    "model = SubtopicGAT(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    num_subtopics=num_subtopics,\n",
    "    edge_emb_dim=16,\n",
    "    heads=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e880682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_model = train_gat_model(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    splits=splits,\n",
    "    num_epochs=1000,\n",
    "    hard_neg_start_epoch = 30,\n",
    "    patience = 20,\n",
    "    lr = .001,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386dc9a",
   "metadata": {},
   "source": [
    "## Predictions & Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47471e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "model_path = 'best_model_final.pt'\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "#TFIDF\n",
    "tfidf = joblib.load('fitted_tfidf_vectorizer.joblib')\n",
    "\n",
    "#UMAP\n",
    "umap_model = joblib.load('fitted_umap_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz, csr_matrix\n",
    "\n",
    "#Load cluster centroids\n",
    "cluster_centroids = load_npz(\"cluster_centroids.npz\")\n",
    "\n",
    "#Transform cluster centroids into UMAP reduced embeddings to match\n",
    "umap_reduced_cluster_centroids = {}\n",
    "for cluster_idx in range(cluster_centroids.shape[0]):\n",
    "    tfidf_centroid_vector_sparse = cluster_centroids[cluster_idx, :]\n",
    "    \n",
    "    tfidf_centroid_vector_dense = tfidf_centroid_vector_sparse.toarray()\n",
    "    \n",
    "    umap_embedding_centroid = umap_model.transform(tfidf_centroid_vector_dense)\n",
    "    \n",
    "    scaled_umap_embedding_centroid = scaler.transform(umap_embedding_centroid)\n",
    "    \n",
    "    umap_reduced_cluster_centroids[cluster_idx] = scaled_umap_embedding_centroid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bertopic_models = {}\n",
    "for cluster_id in range(9):\n",
    "    model_path = f\"Models/bertopic_cluster_{cluster_id}.model\"\n",
    "    \n",
    "    fitted_bertopic_models[cluster_id] = BERTopic.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Analyzing and POS tagging to match tfidf preprocessing\n",
    "def analyze_query(doc_query: spacy.tokens.Doc):\n",
    "    query_nav_lemmas = []\n",
    "    query_entities_strings = []\n",
    "    \n",
    "    for token in doc_query:\n",
    "        if not token.is_space and not token.is_punct:\n",
    "            if token.pos_ in ['Non', 'PROPN', 'ADJ', 'ADV', 'VERB', 'AUX']:\n",
    "                query_nav_lemmas.append(token.lemma_)\n",
    "    \n",
    "    for ent in doc_query.ents:\n",
    "        query_entities_strings.append(str(ent))\n",
    "    \n",
    "    return query_nav_lemmas, query_entities_strings\n",
    "\n",
    "# To match preprocessing\n",
    "def resentece_query(words_list_or_entity_list):\n",
    "    return \"|\".join([w for w in words_list_or_entity_list if len(w.strip()) > 0])\n",
    "\n",
    "def preprocess_lemmatize_query(query_text: str, STOP_WORDS_SET: set) -> str:\n",
    "    \n",
    "    # lowercase and remove anything but letters\n",
    "    query_text = query_text.lower()\n",
    "    \n",
    "    doc = nlp(query_text)\n",
    "    \n",
    "    query_nav_lemmas, query_entities_strings = analyze_query(doc)\n",
    "    \n",
    "    nav_string = resentece_query(query_nav_lemmas)\n",
    "    entities_string = resentece_query(query_entities_strings)\n",
    "    \n",
    "    combined_text = f\"{nav_string or ''}|{entities_string or ''}\"\n",
    "    \n",
    "    words = re.split(r'\\||\\#', combined_text)\n",
    "    \n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        # Remove single character words/parts of a list (a.)\n",
    "        cleaned_word = re.sub(r'\\b[A-Za-z]\\.', '', w)\n",
    "        cleaned_word = re.sub(r'\\b[A-Za-z]\\b', '', cleaned_word)\n",
    "        \n",
    "        # Replace specific unicode whitespace characters with single space, in case query is a wiki article\n",
    "        cleaned_word = re.sub(r'[\\xa0\\u200b\\u202f]', ' ', cleaned_word)\n",
    "        \n",
    "        cleaned_word = cleaned_word.strip()\n",
    "        \n",
    "        if len(cleaned_word) > 1 and cleaned_word not in STOP_WORDS_SET:\n",
    "            processed_words.append(cleaned_word)\n",
    "    \n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_node_feature_vector(\n",
    "    query_text: str,\n",
    "    fitted_tfidf_vectorizer: TfidfVectorizer,\n",
    "    fitted_umap_model: UMAP,\n",
    "    fitted_scaler_node_features: StandardScaler,\n",
    "    fitted_cluster_encoder: OneHotEncoder,\n",
    "    df_global_for_cluster_dim: pd.DataFrame,\n",
    "    STOP_WORDS_SET: set,\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    # Preprocess & Lemmatize\n",
    "    processed_query_string = preprocess_lemmatize_query(query_text, STOP_WORDS_SET)\n",
    "    \n",
    "    # TF-IDF Vectorize the query\n",
    "    query_tfidf_vector = fitted_tfidf_vectorizer.transform([query_text])\n",
    "    \n",
    "    # UMAP Transform\n",
    "    query_umap_embedding = fitted_umap_model.transform(query_tfidf_vector)\n",
    "    \n",
    "    # Standardize\n",
    "    query_scaled_embedding = fitted_scaler_node_features.transform(query_umap_embedding)\n",
    "    \n",
    "    # Cluster One-Hot\n",
    "    cluster_dim = fitted_cluster_encoder.categories_[0].size\n",
    "    query_cluster_onehot = np.zeros((1,cluster_dim))\n",
    "    \n",
    "    # Concatenate\n",
    "    full_query_node_feature_vector = np.hstack([query_scaled_embedding, query_cluster_onehot])\n",
    "    \n",
    "    return full_query_node_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf217eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_related_documents(\n",
    "    query_text: str,\n",
    "    trained_model: torch.nn.Module,\n",
    "    global_data_obj: Data,\n",
    "    original_G_nx: nx.MultiDiGraph,\n",
    "    df_global: pd.DataFrame,\n",
    "    title_to_idx_global: dict,\n",
    "    doc_titles_list: list,\n",
    "    subtopic_to_idx_global: dict,\n",
    "    similarity_df_global: pd.DataFrame,\n",
    "    subtopic_edge_dict_global: dict,\n",
    "    fitted_tfidf_vectorizer: TfidfVectorizer,\n",
    "    fitted_umap_model: UMAP,\n",
    "    fitted_scaler_node_features: StandardScaler,\n",
    "    fitted_cluster_encoder: OneHotEncoder,\n",
    "    umap_reduced_cluster_centroids: dict,\n",
    "    STOP_WORDS: set,\n",
    "    fitted_bertopic_models: dict,\n",
    "    #global_subtopic_centroids_umap: dict,\n",
    "    top_k_results: int = 10,\n",
    "    prediction_probability_threshold: float = 0.5,\n",
    "    include_neighbors: bool =True,\n",
    "    max_neighbors_per_doc: int = 5,\n",
    "    device: str = 'cpu',\n",
    "    query_sim_threshold: float = 0.5\n",
    "):\n",
    "    trained_model.eval()\n",
    "    trained_model.to(device)\n",
    "    \n",
    "    # Copy of Graph\n",
    "    temp_G_nx = original_G_nx.copy()\n",
    "    \n",
    "    num_existing_nodes = global_data_obj.num_nodes\n",
    "    query_node_id = num_existing_nodes\n",
    "    \n",
    "    query_subtopics_list = []\n",
    "    \n",
    "    query_node_features_np = get_query_node_feature_vector(\n",
    "        query_text=query_text,\n",
    "        fitted_tfidf_vectorizer=fitted_tfidf_vectorizer,\n",
    "        fitted_umap_model=fitted_umap_model,\n",
    "        fitted_scaler_node_features=fitted_scaler_node_features,\n",
    "        fitted_cluster_encoder=fitted_cluster_encoder,\n",
    "        df_global_for_cluster_dim=df,\n",
    "        STOP_WORDS_SET=STOP_WORDS\n",
    "    )\n",
    "    query_node_features_tensor = torch.tensor(query_node_features_np, dtype=torch.float).to(device)\n",
    "    \n",
    "    query_scaled_embedding_np = query_node_features_np[:, :fitted_umap_model.n_components]\n",
    "    \n",
    "    # Assign query to cluster via centroid similarity\n",
    "    best_cluster = None\n",
    "    highest_similarity_to_centroid = -1.0\n",
    "    \n",
    "    for cluster_id, centroid_embedding_umap in umap_reduced_cluster_centroids.items():\n",
    "        similarity = cosine_similarity(query_scaled_embedding_np, centroid_embedding_umap)[0][0]\n",
    "        \n",
    "        if similarity > highest_similarity_to_centroid:\n",
    "            highest_similarity_to_centroid = similarity\n",
    "            best_cluster = cluster_id\n",
    "    \n",
    "    print(f\"Query '{query_text}' is most similar to cluster: {best_cluster} (Similarity: {highest_similarity_to_centroid:.4f})\")\n",
    "    \n",
    "    query_subtopics_list = []\n",
    "    \n",
    "    bertopic_model_assigned_topic = False\n",
    "    if best_cluster is not None and fitted_bertopic_models.get(best_cluster) is not None:\n",
    "        bertopic_model_for_query = fitted_bertopic_models[best_cluster]\n",
    "        \n",
    "        query_topic_ids, _ = bertopic_model_for_query.transform([query_text])\n",
    "        \n",
    "        if query_topic_ids and query_topic_ids[0] != -1:\n",
    "            full_subtopic_name = f\"{best_cluster}_{query_topic_ids[0]}\"\n",
    "            \n",
    "            if full_subtopic_name in subtopic_to_idx_global:\n",
    "                query_subtopics_list.append(full_subtopic_name)\n",
    "                bertopic_model_assigned_topic = True\n",
    "            else:\n",
    "                print(f\"Warning: BERTopic topic '{full_subtopic_name}' not found in global subtopic_to_idx. Skipping.\")\n",
    "    \n",
    "        else:\n",
    "            print(\"No specific topic assigned by BERTopic for this query (id -1)\")\n",
    "    else:\n",
    "        print(f\"Warning: No BERTopic model found for predicted cluster\")\n",
    "            \n",
    "    # Final check before candidate edges\n",
    "    if not query_subtopics_list:\n",
    "        print(\"Query could not be assigned to any meaningful subtopics, returning empty list\")\n",
    "        return []\n",
    "    \n",
    "    # Generate candidate Edges\n",
    "    target_docs_in_cluster_df = df_global[df_global['cluster'] == best_cluster]\n",
    "    \n",
    "    filtered_target_doc_ids = [title_to_idx_global[title] for title in target_docs_in_cluster_df['title']]\n",
    "    \n",
    "    if not filtered_target_doc_ids:\n",
    "        print(f\"No documents found in predicted cluster {best_cluster}\")\n",
    "        return []\n",
    "    \n",
    "    candidate_edges_sources = []\n",
    "    candidate_edges_targets = []\n",
    "    candidate_edges_weights = []\n",
    "    candidate_edge_sims = []\n",
    "    candidate_edge_subtopic_indices = []\n",
    "    \n",
    "    # Only search documents inside the cluster\n",
    "    for target_doc_id in filtered_target_doc_ids:\n",
    "        target_doc_title = doc_titles_list[target_doc_id]\n",
    "        target_doc_subtopics_list = df_global.loc[df_global['title'] == target_doc_title, 'topic_list'].values[0]\n",
    "        \n",
    "        best_sim_for_query_target = -1.0\n",
    "        best_subtopic_pair_for_query_target = (None, None)\n",
    "        \n",
    "        # Find best subtopic pari between query's extracted subtopics and target document\n",
    "        if query_subtopics_list and target_doc_subtopics_list:\n",
    "            for qs_query_topic in query_subtopics_list:\n",
    "                for ts_doc_topic in target_doc_subtopics_list:\n",
    "                    sim_val = similarity_df_global.get(qs_query_topic, {}).get(ts_doc_topic)\n",
    "                    if sim_val is not None:\n",
    "                        if sim_val > best_sim_for_query_target:\n",
    "                            best_sim_for_query_target = sim_val\n",
    "                            best_subtopic_pair_for_query_target = (qs_query_topic, ts_doc_topic)\n",
    "                            \n",
    "        sim_val_final = best_sim_for_query_target if best_sim_for_query_target >= 0 else 0.0\n",
    "    \n",
    "            \n",
    "        weight_val = subtopic_edge_dict_global.get(best_subtopic_pair_for_query_target, 0.0)\n",
    "        \n",
    "        candidate_edges_sources.append(query_node_id)\n",
    "        candidate_edges_targets.append(target_doc_id)\n",
    "        candidate_edges_weights.append(weight_val)\n",
    "        candidate_edge_sims.append(sim_val_final)\n",
    "        candidate_edge_subtopic_indices.append([\n",
    "            subtopic_to_idx_global.get(best_subtopic_pair_for_query_target[0], 0),\n",
    "            subtopic_to_idx_global.get(best_subtopic_pair_for_query_target[1], 0)\n",
    "        ])\n",
    "    if not candidate_edges_sources: # No relevant docs above threshold\n",
    "        print(f\"No relevant documents found for the query above the similarity threshold\")\n",
    "        return []\n",
    "    # Create a temporary expanded PyG object for creation\n",
    "    expanded_x = torch.cat([global_data_obj.x, query_node_features_tensor], dim=0).to(device)\n",
    "    \n",
    "    candidate_edge_index = torch.tensor([candidate_edges_sources, candidate_edges_targets], dtype=torch.long)\n",
    "    candidate_edge_weight = torch.tensor(candidate_edges_weights, dtype=torch.float)\n",
    "    candidate_edge_sim = torch.tensor(candidate_edge_sims, dtype=torch.float)\n",
    "    candidate_edge_subtopic = torch.tensor(candidate_edge_subtopic_indices, dtype=torch.long)\n",
    "    \n",
    "    expanded_edge_index = torch.cat([global_data_obj.edge_index, candidate_edge_index], dim=1).to(device)\n",
    "    expanded_edge_weight = torch.cat([global_data_obj.edge_weight, candidate_edge_weight], dim=0).to(device)\n",
    "    expanded_edge_sim = torch.cat([global_data_obj.edge_sim, candidate_edge_sim], dim=0).to(device)\n",
    "    expanded_edge_subtopic = torch.cat([global_data_obj.edge_subtopic, candidate_edge_subtopic], dim=0).to(device)\n",
    "    \n",
    "    temp_data_for_prediction = Data(\n",
    "        x=expanded_x,\n",
    "        edge_index=expanded_edge_index,\n",
    "        edge_weight=expanded_edge_weight,\n",
    "        edge_sim=expanded_edge_sim,\n",
    "        edge_subtopic=expanded_edge_subtopic\n",
    "    ).to(device)\n",
    "    \n",
    "    # Link Prediction with trained model\n",
    "    with torch.no_grad():\n",
    "        query_to_doc_preds_logits = trained_model(\n",
    "            x=temp_data_for_prediction.x,\n",
    "            edge_index=temp_data_for_prediction.edge_index,\n",
    "            edge_subtopic=temp_data_for_prediction.edge_subtopic,\n",
    "            edge_weight=temp_data_for_prediction.edge_weight,\n",
    "            edge_sim=temp_data_for_prediction.edge_sim\n",
    "        )\n",
    "        query_to_doc_preds_probs = torch.sigmoid(query_to_doc_preds_logits).cpu().numpy()\n",
    "        \n",
    "    # Ranking and Retreival\n",
    "    results = []\n",
    "\n",
    "    start_index_of_query_edges = global_data_obj.edge_index.size(1)\n",
    "    end_index_of_query_edges = start_index_of_query_edges + len(candidate_edges_sources)\n",
    "    \n",
    "    query_doc_probs_filtered_for_ranking = query_to_doc_preds_probs[start_index_of_query_edges:end_index_of_query_edges]\n",
    "    query_doc_targets_filtered_for_ranking = torch.tensor(candidate_edges_targets, dtype=torch.long).cpu().numpy()\n",
    "    \n",
    "    all_query_doc_predictions = []\n",
    "    for i, prob in enumerate(query_doc_probs_filtered_for_ranking):\n",
    "        target_doc_global_id = query_doc_targets_filtered_for_ranking[i]\n",
    "        target_doc_title = doc_titles_list[target_doc_global_id]\n",
    "        \n",
    "        all_query_doc_predictions.append({\n",
    "            'doc_title': target_doc_title,\n",
    "            'prediction_probability': prob,\n",
    "            'node_id': target_doc_global_id,\n",
    "            'source_type': 'direct_prediction'\n",
    "        })\n",
    "    all_query_doc_predictions.sort(key=lambda x: x['prediction_probability'], reverse=True)\n",
    "    \n",
    "    final_unique_results = []\n",
    "    seen_doc_ids = set()\n",
    "    \n",
    "    for res in all_query_doc_predictions:\n",
    "        #Stop if enough direct predicions to meet top k resutls\n",
    "        if len(final_unique_results) >= top_k_results and not include_neighbors:\n",
    "            break\n",
    "        \n",
    "        if res['prediction_probability'] >= prediction_probability_threshold and \\\n",
    "           res['node_id'] not in seen_doc_ids:\n",
    "            \n",
    "            final_unique_results.append(res)\n",
    "            seen_doc_ids.add(res['node_id'])\n",
    "            \n",
    "    # Graph Traversal\n",
    "    traversal_results = []\n",
    "    if include_neighbors and len(seen_doc_ids) < top_k_results:\n",
    "        print(\"Traversing Graph for Related Documents\")\n",
    "        for direct_res in final_unique_results:\n",
    "            source_doc_id = direct_res['node_id']\n",
    "            source_doc_title = direct_res['doc_title']\n",
    "            \n",
    "            current_neighbors_added_from_this_doc = 0\n",
    "            # Out Neigbors\n",
    "            for neighbor_title in temp_G_nx.successors(source_doc_title):\n",
    "                neighbor_id = title_to_idx_global.get(neighbor_title)\n",
    "                if neighbor_id is not None and neighbor_id not in seen_doc_ids:\n",
    "                    traversal_results.append({\n",
    "                        'doc_title': neighbor_title,\n",
    "                        'prediction_probability': 0.0, #placeholder\n",
    "                        'node_id': neighbor_id,\n",
    "                        'source_type': f\"out_neighbor_from_id_{source_doc_id}\"\n",
    "                    })\n",
    "                    current_neighbors_added_from_this_doc +=1\n",
    "                    seen_doc_ids.add(neighbor_id)\n",
    "                if len(final_unique_results) + len(traversal_results) >= top_k_results: break\n",
    "                    \n",
    "            current_neighbors_added_from_this_doc = 0\n",
    "            # in Neighbors\n",
    "            for neighbor_title in temp_G_nx.predecessors(source_doc_title):\n",
    "                neighbor_id = title_to_idx_global.get(neighbor_title)\n",
    "                if neighbor_id is not None and neighbor_id not in seen_doc_ids:\n",
    "                    traversal_results.append({\n",
    "                        'doc_title': neighbor_title,\n",
    "                        'prediction_probability': 0.0, #placeholder\n",
    "                        'node_id': neighbor_id,\n",
    "                        'source_type': f\"out_neighbor_from_id_{source_doc_id}\"\n",
    "                    })\n",
    "                    current_neighbors_added_from_this_doc +=1\n",
    "                    seen_doc_ids.add(neighbor_id)\n",
    "                    if len(traversal_results) >= max_neighbors_per_doc: break #limit neighbors\n",
    "                if len(final_unique_results) + len(traversal_results) >= top_k_results: break\n",
    "    final_results = final_unique_results + traversal_results\n",
    "            \n",
    "    final_results.sort(key=lambda x: x['prediction_probability'], reverse=True)\n",
    "            \n",
    "    print(f\"\\nTop {top_k_results} related documents for query: '{query_text}'\")\n",
    "    for i, res in enumerate(final_results[:top_k_results]):\n",
    "        print(f\"{i+1}. Document: '{res['doc_title']}' (ID: {res['node_id']}) - Probability: {res['prediction_probability']:.4f} (Source: {res['source_type']})\")\n",
    "    \n",
    "    return results[:top_k_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = \"Alcohol increases the risk of cancer of the breast (in women), throat, liver, oesophagus, mouth, larynx, and colon.[58] In Western Europe, 10% of cancers in males and 3% of cancers in females are attributed to alcohol exposure, especially liver and digestive tract cancers.[59] Cancer from work-related substance exposures may cause between 2 and 20% of cases,[60] causing at least 200,000 deaths.[61] Cancers such as lung cancer and mesothelioma can come from inhaling tobacco smoke or asbestos fibers, or leukemia from exposure to benzene\"\n",
    "prediction_results = predict_related_documents(\n",
    "    query_text = my_query,\n",
    "    trained_model = model,\n",
    "    global_data_obj = data,\n",
    "    original_G_nx = G,\n",
    "    df_global = df,\n",
    "    title_to_idx_global = title_to_idx,\n",
    "    doc_titles_list = doc_titles,\n",
    "    subtopic_to_idx_global = subtopic_to_idx,\n",
    "    similarity_df_global = similarity_df,\n",
    "    subtopic_edge_dict_global = subtopic_edge_dict,\n",
    "    fitted_tfidf_vectorizer = tfidf,\n",
    "    fitted_umap_model = umap_model,\n",
    "    fitted_scaler_node_features = scaler,\n",
    "    fitted_cluster_encoder = encoder,\n",
    "    STOP_WORDS = STOP_WORDS,\n",
    "    umap_reduced_cluster_centroids=umap_reduced_cluster_centroids,\n",
    "    fitted_bertopic_models = fitted_bertopic_models,\n",
    "    top_k_results = 100,\n",
    "    prediction_probability_threshold = 0.5,\n",
    "    include_neighbors=True,\n",
    "    max_neighbors_per_doc = 5,\n",
    "    device = 'cpu',\n",
    "    query_sim_threshold = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e06128",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import softmax as pyg_softmax\n",
    "\n",
    "class EAGATConv(MessagePassing):\n",
    "    def __init__(self, in_channels, hidden_channels, edge_emb_dim=16, heads=2, dropout=0.3):\n",
    "        super().__init__(aggr='add')\n",
    "        self.heads = heads\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_node = Linear(in_channels, hidden_channels * heads, bias=False)\n",
    "        self.lin_edge = Linear(2 * edge_emb_dim + 2, hidden_channels * heads, bias=False)\n",
    "        self.att = Linear(3 * hidden_channels * heads, 1, bias=False)\n",
    "        self.leaky_relu = LeakyReLU(0.2)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.lin_node(x)\n",
    "        edge_attr = self.lin_edge(edge_attr)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr, index, ptr, size_i):\n",
    "        x_cat = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "        alpha = self.att(x_cat)\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        alpha = pyg_softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class SubtopicEAGAT(torch.nn.Module):\n",
    "    # Parameters = default values\n",
    "    def __init__(self, in_channels, hidden_channels, num_subtopics, edge_emb_dim=16, heads=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Subtopic pair embedding\n",
    "        self.subtopic_embed = nn.Embedding(num_subtopics,edge_emb_dim)\n",
    "\n",
    "        self.lin_edge = nn.Linear(2*edge_emb_dim+2, edge_emb_dim)\n",
    "\n",
    "        # EGAT Layer\n",
    "        self.egat1 = EGATConv(in_channels,hidden_channels,heads=heads,edge_emb_dim=edge_emb_dim, dropout=dropout)\n",
    "        self.egat2 = EGATConv(hidden_channels * heads, hidden_channels, heads=1,edge_emb_dim=edge_emb_dim, dropout=dropout)\n",
    "\n",
    "        #Decoder MLP for edge scoring\n",
    "        self.edge_decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels + 2 * edge_emb_dim + 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_subtopic, edge_weight, edge_sim):\n",
    "        # Encode Node Features\n",
    "\n",
    "        s1 = self.subtopic_embed(edge_subtopic[:,0])\n",
    "        s2 = self.subtopic_embed(edge_subtopic[:,1])\n",
    "        edge_feats = torch.cat([s1,s2, edge_sim.unsqueeze(1), edge_weight.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        x = self.egat1(x, edge_index, edge_feats).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.egat2(x, edge_index, edge_feats)\n",
    "\n",
    "        # Compute edge-wise node embeddings\n",
    "        src, tgt = edge_index\n",
    "        h_src = x[src]\n",
    "        h_tgt = x[tgt]\n",
    "\n",
    "        edge_repr = torch.cat([h_src, h_tgt, edge_feats], dim=-1)\n",
    "        return self.edge_decoder(edge_repr).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33003ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = data.x.shape[1]\n",
    "hidden_channels = 64          \n",
    "num_subtopics   = len(subtopic_to_idx)\n",
    "\n",
    "model = SubtopicEAGAT(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    num_subtopics=num_subtopics,\n",
    "    edge_emb_dim=32,  \n",
    "    heads=2,           \n",
    "    dropout=0.5       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_gat_model(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    splits=splits,\n",
    "    num_epochs=1000,\n",
    "    hard_neg_start_epoch = 30,\n",
    "    patience = 15,\n",
    "    lr = .0001,\n",
    "    device='cpu'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
