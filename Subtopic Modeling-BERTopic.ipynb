{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import HdpModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "import regex as re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca948f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustered Diseases db from k-means\n",
    "con = sqlite3.connect(\"clustered_diseases.db\")\n",
    "\n",
    "df= pd.read_sql_query(\"SELECT * from clustered_diseases\", con)\n",
    "con.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in tqdm(df.iterrows(), total=len(df)):\n",
    "    \n",
    "    #Filter Wikipedia-related elements\n",
    "    #text = re.sub(r'\\[.*\\]', '', text) \n",
    "    text = re.sub(r'^From Wikipedia, the free encyclopedia', '', str(r[\"text\"]), flags=re.MULTILINE)\n",
    "    text = re.sub(r'Retrieved from.*\\s.*', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text) \n",
    "    text = re.sub(r'^References.*', ' ', text, flags=re.MULTILINE | re.DOTALL)\n",
    "    text = re.sub(r'^Citations.*', ' ', text, flags=re.MULTILINE | re.DOTALL)\n",
    "    text = re.sub(r'^Footnotes.*', ' ', text, flags=re.MULTILINE | re.DOTALL)\n",
    "    #text = re.sub(r'\\n', ' ', text)  # Want to keep \\n to split into paragraphs\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    text = re.sub(r'[\\xa0\\u200b\\u202f]', ' ', text) # Removes extra artifacts\n",
    "    text = re.sub(r'\\b[A-Za-z]\\.',' ', text) #Removes letters in a list like a. \n",
    "    #text = re.sub(r'[^\\w\\s]', '', text) # Removes punctuation (all non text, non whitespace)\n",
    "    \n",
    "    df.at[i, \"text\"] = text\n",
    "    \n",
    "    #Create a column with stopwords removed\n",
    "    df.at[i, \"text_no_SW\"] =  \" \".join([\n",
    "        word.lower() for word in df.at[i, \"text\"].split() if word.lower() not in STOP_WORDS])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    # You can tune this depending on formatting\n",
    "    raw_paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    \n",
    "    # Merge small paragraphs (headers) into bigger ones based on sentence count\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    \n",
    "    for p in raw_paragraphs:\n",
    "        buffer += ' ' + p if buffer else p\n",
    "        if len(sent_tokenize(buffer)) >= 2:\n",
    "            paragraphs.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer.strip())\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75862cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    paragraphs = split_paragraphs(row['text'])\n",
    "    for para in paragraphs:\n",
    "        rows.append({\n",
    "            'title' : row['title'],\n",
    "            'cluster' : row['cluster'],\n",
    "            'paragraph' : para\n",
    "        })\n",
    "        \n",
    "paragraph_df = pd.DataFrame(rows)\n",
    "paragraph_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a single sentence\n",
    "def analyze_sentence(sent):\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    lemmas = []\n",
    "    nav = []\n",
    "\n",
    "    for token in sent:\n",
    "        \n",
    "        lemmas.append(token.lemma_)\n",
    "        \n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "           nouns.append(token.lemma_ )\n",
    "           nav.append(token.lemma_ )\n",
    "        if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "            adjectives.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbs.append(token.lemma_ )\n",
    "            nav.append(token.lemma_)\n",
    "\n",
    "    return (nouns, adjectives, verbs, lemmas, nav,\n",
    "            [str(e) for e in sent.ents], [str(nc) for nc in sent.noun_chunks])\n",
    "\n",
    "\n",
    "\n",
    "# Rejoin the individual parts of the sentences\n",
    "def resentence(words):\n",
    "    ###\n",
    "    # You cannot join sentences with \".\", because this can also be part of an entity\n",
    "    # With \"#\" this is less likely\n",
    "    # You can join words with \"|\"\n",
    "    # Empty spaces are ignored\n",
    "    ###\n",
    "    return \"#\".join([\"|\".join([w for w in sent_words if len(w) > 0])\n",
    "                     for sent_words in words if len(sent_words) > 0])\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraph(i, text, STOP_WORDS):\n",
    "    import spacy\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    nouns, adjectives, verbs, lemmas, nav = [], [], [], [], []\n",
    "    entities, noun_chunks = [], []\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        (sent_nouns, sent_adjectives, sent_verbs, sent_lemmas,\n",
    "        sent_nav, sent_entities, sent_noun_chunks) = analyze_sentence(sentence)\n",
    "\n",
    "        nouns.append(sent_nouns)\n",
    "        adjectives.append(sent_adjectives)\n",
    "        verbs.append(sent_verbs)\n",
    "        nav.append(sent_nav)\n",
    "        lemmas.append(sent_lemmas)\n",
    "        entities.append(sent_entities)\n",
    "        noun_chunks.append(sent_noun_chunks)\n",
    "\n",
    "    data = {\n",
    "        \"index\": i,\n",
    "        \"nouns\": resentence(nouns),\n",
    "        \"adjectives\": resentence(adjectives),\n",
    "        \"verbs\": resentence(verbs),\n",
    "        \"lemmas\": resentence(lemmas),\n",
    "        \"nav\": resentence(nav),\n",
    "        \"entities\": resentence(entities),\n",
    "        \"noun_chunks\": resentence(noun_chunks),\n",
    "        \"no_tokens\": len(resentence(lemmas).split(\"|\")),\n",
    "        \"no_sentences\": len(lemmas),\n",
    "        \"no_noun_chunks\": len(resentence(noun_chunks).split(\"|\")),\n",
    "        \"para_no_SW\": \" \".join([\n",
    "            word for word in text.split() if word.lower() not in STOP_WORDS\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_paragraph)(i, row['paragraph'], STOP_WORDS)\n",
    "    for i, row in tqdm(paragraph_df.iterrows(), total=len(paragraph_df))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004833ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results).set_index(\"index\")\n",
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = sqlite3.connect(\"para_lemmas.db\")\n",
    "result_df.to_sql(\"para_lemmas\", sql, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12962fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df.update(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = sqlite3.connect(\"para_df.db\")\n",
    "paragraph_df.to_sql(\"para_df\", sql, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f53eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(text):\n",
    "    words = re.split(r'\\||\\#', text)\n",
    "    return [w.lower() for w in words if len(w)>1 and w.lower() not in STOP_WORDS]\n",
    "\n",
    "def get_gensim_text(paragraph_df, cluster):\n",
    "    #Filter to only include specific cluster\n",
    "    df_cluster = paragraph_df[paragraph_df['cluster'] == cluster]\n",
    "    \n",
    "    #Combine nav and entities columns into one list of words \n",
    "    text_list = []\n",
    "    for _, row in df_cluster.iterrows():\n",
    "        nav_text = filter_words(row['nav']) if pd.notna(row['nav']) else []\n",
    "        entity_text = filter_words(row['entities']) if pd.notna(row['entities']) else []\n",
    "        text_list.append(nav_text + entity_text)\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(model, text, dictionary):\n",
    "    \n",
    "    #Function to show coherence score of a model\n",
    "    \n",
    "    coherence = CoherenceModel(model=model,\n",
    "                               texts = text,\n",
    "                               dictionary = dictionary,\n",
    "                               coherence = 'c_v',\n",
    "                               processes=1)\n",
    "    coherence_score = coherence.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72253b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_filtered_text(row):\n",
    "    nav_text = filter_words(row['nav']) if pd.notna(row['nav']) else []\n",
    "    entity_text = filter_words(row['entities']) if pd.notna(row['entities']) else []\n",
    "    return nav_text + entity_text\n",
    "\n",
    "paragraph_df['filtered_text'] = paragraph_df.apply(combine_filtered_text, axis=1)\n",
    "\n",
    "paragraph_df['filtered_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lemmas(row):\n",
    "    lemmas = filter_words(row['lemmas']) if pd.notna(row['lemmas']) else []\n",
    "    return lemmas\n",
    "\n",
    "paragraph_df['filtered_lemmas '] = paragraph_df.apply(filter_lemmas, axis=1)\n",
    "\n",
    "paragraph_df['filtered_lemmas'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66767265",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_diversity(model, top_n=10):\n",
    "    topics = model.get_topics()\n",
    "    all_words = []\n",
    "    \n",
    "    for topic_id, words in topics.items():\n",
    "        if topic_id == -1:  # exclude outliers\n",
    "            continue\n",
    "        top_words = [word for word, _ in words[:top_n]]\n",
    "        all_words.extend(top_words)\n",
    "        \n",
    "    unique_words = set(all_words)\n",
    "    diversity = len(unique_words) / len(all_words) if all_words else 0\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81972238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(model, topic_id, n=3):\n",
    "    \" Get top N words for topics from BERTopic models \"\n",
    "    words = model.get_topic(topic_id)\n",
    "    return ', '.join([word for word, _ in words[:n]]) if words else \"No Topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfe683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_short_paragraphs(df, max_sentences=2):\n",
    "    df = df.sort_values(['title']).reset_index(drop=True)\n",
    "    merged_rows = []\n",
    "    skip_indices = set()\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i in skip_indices:\n",
    "            # This paragraph has already been merged, skip\n",
    "            continue\n",
    "        \n",
    "        current = df.loc[i]\n",
    "        if current.no_sentences > max_sentences: \n",
    "            # Paragraph is long enough, no changes needed\n",
    "            merged_rows.append(current.to_dict())\n",
    "            continue\n",
    "        \n",
    "        # Identify neighbors with same title\n",
    "        prev_idx = i-1 if i > 0 else None\n",
    "        next_idx = i+1 if i < len(df) - 1 else None\n",
    "        \n",
    "        prev_row = df.loc[prev_idx] if prev_idx is not None and df.loc[prev_idx, 'title'] == current.title else None\n",
    "        next_row = df.loc[next_idx] if next_idx is not None and df.loc[next_idx, 'title'] == current.title else None\n",
    "        \n",
    "        # Choose neighbor to merge with (Shorter of the two if there are two)\n",
    "        candidates = []\n",
    "        if prev_row is not None and prev_idx not in skip_indices:\n",
    "            candidates.append(('prev', prev_row.no_sentences))\n",
    "        if next_row is not None and next_idx not in skip_indices:\n",
    "            candidates.append(('next', next_row.no_sentences))\n",
    "            \n",
    "        if not candidates:\n",
    "            # No neighbors to merge with, keep as is\n",
    "            merged_rows.append(current.to_dict())\n",
    "            continue\n",
    "        \n",
    "        # Find neighbor with smallest no_sentences\n",
    "        best_side = min(candidates, key=lambda x:x[1])[0]\n",
    "        \n",
    "        if best_side == 'prev':\n",
    "            neighbor = prev_row\n",
    "            neighbor_idx = prev_idx\n",
    "            merged_text = neighbor.doc_str_lem + \" \" + current.doc_str_lem\n",
    "            merged_no_sentences = neighbor.no_sentences + current.no_sentences\n",
    "            merged_index = min(neighbor['index'], current['index'])\n",
    "            \n",
    "        else:\n",
    "            neighbor = next_row\n",
    "            neighbor_idx = next_idx\n",
    "            merged_text = current.doc_str_lem + \" \" + neighbor.doc_str_lem\n",
    "            merged_no_sentences = current.no_sentences + neighbor.no_sentences\n",
    "            merged_index = min(current['index'], neighbor['index'])\n",
    "            \n",
    "        merged_paragraph = current.to_dict()\n",
    "        merged_paragraph.update({\n",
    "            'doc_str_lem'  : merged_text,\n",
    "            'no_sentences' : merged_no_sentences,\n",
    "            'index'        : merged_index\n",
    "        })\n",
    "        \n",
    "        merged_rows.append(merged_paragraph)\n",
    "        skip_indices.add(neighbor_idx) # Skip merged neighbor\n",
    "        \n",
    "    merged_df = pd.DataFrame(merged_rows).sort_values(['title','index']).reset_index(drop=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraph_df.shape)\n",
    "\n",
    "print(f\"Average number of sentences per paragraph: {paragraph_df['no_sentences'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_short_paragraphs(paragraph_df, max_sentences=2)\n",
    "\n",
    "print(merged_df.shape)\n",
    "\n",
    "print(f\"Average number of sentences per paragraph: {merged_df['no_sentences'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['filtered_lemmas'] = merged_df['doc_str_lem'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-V2')\n",
    "corpus_embeddings = model.encode(merged_df['doc_str_lem'].tolist(), show_progress_bar=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import MaximalMarginalRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f23265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_model(topic_model, docs, lemmas):\n",
    "    \n",
    "    all_topics = topic_model.get_topics()\n",
    "    topic_words = []\n",
    "    sorted_topic_ids = sorted([t for t in all_topics.keys()])\n",
    "    for topic_id in sorted_topic_ids:\n",
    "        words = [word for word, prob in topic_model.get_topic(topic_id)]\n",
    "        topic_words.append(words)\n",
    "    dictionary = Dictionary(lemmas)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.7)\n",
    "    coherence_model = CoherenceModel(topics=topic_words, texts=lemmas, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def run_hdbscan_grid(cluster_docs, cluster_lemmas, cluster_embeddings, param_grid,\n",
    "                     embedding_model, umap_model, vectorizer_model, representation_model):\n",
    "    best_model = None\n",
    "    best_coherence = -1\n",
    "    best_params = None\n",
    "    best_topics = None\n",
    "    \n",
    "    for min_cluster_size, min_samples in tqdm(param_grid, desc=f\"Grid search cluster {cluster_label}\", leave=False):\n",
    "        hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "        \n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            representation_model=representation_model,\n",
    "            verbose=False\n",
    "        )\n",
    "        try:\n",
    "            topics, _ = topic_model.fit_transform(cluster_docs, cluster_embeddings)\n",
    "            outliers = sum(1 for t in topics if t == -1)\n",
    "            if outliers >= len(cluster_docs) * 0.5:\n",
    "                continue # Skip bad models with too many outliers\n",
    "            coherence = evaluate_topic_model(topic_model, cluster_docs, cluster_lemmas)\n",
    "            if coherence > best_coherence:\n",
    "                best_model = topic_model\n",
    "                best_coherence = coherence\n",
    "                best_topics = topics\n",
    "                best_params = {'min_cluster_size': min_cluster_size,\n",
    "                               'min_samples' : min_samples\n",
    "                              }\n",
    "        except Exception as e:\n",
    "            time.sleep(1) # In case of hugging face rate limit\n",
    "            continue\n",
    "    return best_model, best_topics, best_coherence, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static Models\n",
    "umap_model = UMAP(random_state=42)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,2))\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "\n",
    "# HDBSCAN parameter grid\n",
    "hdbscan_params = list(product(range(5,16), [1,5,10,15])) #min_cluster_size x min_samples\n",
    "\n",
    "results = []\n",
    "\n",
    "for cluster_label in tqdm(merged_df['cluster'].unique()):\n",
    "    cluster_mask = merged_df['cluster'] == cluster_label\n",
    "    \n",
    "    #For BERTopic\n",
    "    cluster_docs = merged_df.loc[cluster_mask, 'doc_str_lem'].tolist()\n",
    "    \n",
    "    #For gensim\n",
    "    cluster_lemmas = merged_df.loc[cluster_mask, 'filtered_lemmas'].tolist()\n",
    "    cluster_embeddings = corpus_embeddings[cluster_mask]\n",
    "    \n",
    "    best_model, best_topics, best_coherence, best_params = run_hdbscan_grid(\n",
    "        cluster_docs, cluster_lemmas, cluster_embeddings,\n",
    "        param_grid=hdbscan_params,\n",
    "        embedding_model=model,\n",
    "        umap_model=umap_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"cluster\"   : cluster_label,\n",
    "        \"model\"     : best_model,\n",
    "        \"topics\"    : best_topics,\n",
    "        \"coherence\" : best_coherence,\n",
    "        \"params\"    : best_params\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9490e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(f\"\\nCluster {result['cluster']}\")\n",
    "    print(\"Topics:\", len(set(result['topics'])))\n",
    "    print(\"Best Models:\")\n",
    "    print(f\" - Best Coherence Scores: {result['coherence']:.4f}\")\n",
    "    print(f\" - Topic Diversity: {calculate_topic_diversity(result['model']):.4f}\")\n",
    "    print(\" - Best Parameters:\")\n",
    "    for param_name, param_value in result['params'].items():\n",
    "        print(f\"  - {param_name}: {param_value}\")\n",
    "        \n",
    "#Saving Results Dictionary, for MRR = 0.5\n",
    "MRR_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9abc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Models From Original Search Grid\n",
    "\n",
    "#Static Models\n",
    "umap_model = UMAP(random_state=42)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,2))\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "\n",
    "cluster_params = {\n",
    "    0: {\"min_cluster_size\": 6, \"min_samples\": 5},\n",
    "    1: {\"min_cluster_size\": 5, \"min_samples\": 5},\n",
    "    2: {\"min_cluster_size\": 7, \"min_samples\": 1},\n",
    "    3: {\"min_cluster_size\": 6, \"min_samples\": 5},\n",
    "    4: {\"min_cluster_size\": 5, \"min_samples\": 5},\n",
    "    5: {\"min_cluster_size\": 5, \"min_samples\": 5},\n",
    "    6: {\"min_cluster_size\": 5, \"min_samples\": 5},\n",
    "    7: {\"min_cluster_size\": 7, \"min_samples\": 1},\n",
    "    8: {\"min_cluster_size\": 10, \"min_samples\": 15}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cluster_label in tqdm(merged_df['cluster'].unique()):\n",
    "    cluster_mask = merged_df['cluster'] == cluster_label\n",
    "    \n",
    "    cluster_docs = merged_df.loc[cluster_mask, 'doc_str_lem'].tolist()\n",
    "    cluster_lemmas = merged_df.loc[cluster_mask, 'filtered_lemmas'].tolist()\n",
    "    cluster_embeddings = corpus_embeddings[cluster_mask]\n",
    "    \n",
    "    params = cluster_params[cluster_label]\n",
    "    \n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=params['min_cluster_size'],\n",
    "        min_samples=params['min_samples'],\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(cluster_docs, cluster_embeddings)\n",
    "    \n",
    "    #coherence = evaluate_topic_model(topic_model, cluster_docs, cluster_lemmas)\n",
    "    \n",
    "    results[cluster_label] = {\n",
    "        'cluster' : cluster_label,\n",
    "        'model': topic_model,\n",
    "        \"topics\": topics,\n",
    "        #\"coherence\" : coherence,\n",
    "        \"params\" : params\n",
    "    }\n",
    "    \n",
    "    #topic_model.save(f\"Models/bertopic_cluster_{cluster_label}_redo.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b19cf7",
   "metadata": {},
   "source": [
    "## Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_cluster_centroids(df, embeddings):\n",
    "    cluster_topic_embeddings = defaultdict(list)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        topic_id = row['topic_number']\n",
    "        cluster = row['cluster']\n",
    "        \n",
    "        if isinstance(topic_id,str) and not topic_id.endswith('_-1'):\n",
    "            key = (cluster, topic_id)\n",
    "            cluster_topic_embeddings[key].append(embeddings[idx])\n",
    "            \n",
    "    centroids = {\n",
    "        key : np.mean(vectors, axis=0)\n",
    "        for key, vectors in cluster_topic_embeddings.items()\n",
    "    \n",
    "    }\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "def reassign_outliers_by_cluster(df, embeddings, centroids, similarity_threshold=0.7):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for idx, row in df[df['topic_number'].astype(str).str.endswith('_-1')].iterrows():\n",
    "        emb = embeddings[idx].reshape(1,-1)\n",
    "        cluster = row['cluster']\n",
    "        best_topic = None\n",
    "        best_score = -1\n",
    "        \n",
    "        # Only look at centroids from the same cluster\n",
    "        for (cl, topic_id), centroid in centroids.items():\n",
    "            if cl != cluster:\n",
    "                continue\n",
    "            sim = cosine_similarity(emb, centroid.reshape(1,-1))[0][0]\n",
    "            if sim > best_score:\n",
    "                best_score = sim\n",
    "                best_topic = topic_id\n",
    "                \n",
    "        if best_score >= similarity_threshold:\n",
    "            df.at[idx, 'topic_number'] = best_topic\n",
    "            label_mode = df[df['topic_number'] == best_topic]['topic_label'].mode()\n",
    "            if not label_mode.empty:\n",
    "                df.at[idx,'topic_label'] = label_mode[0]\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_centroids(df, embeddings):\n",
    "    topic_embeddings = defaultdict(list)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        topic_id = row['topic_number']        \n",
    "        if isinstance(topic_id,str) and not topic_id.endswith('_-1'):\n",
    "            topic_embeddings[topic_id].append(embeddings[idx])\n",
    "            \n",
    "    centroids = {\n",
    "        topic: np.mean(vectors, axis=0)\n",
    "        for topic, vectors in topic_embeddings.items()\n",
    "    \n",
    "    }\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "def reassign_outliers_globally(df, embeddings, centroids, similarity_threshold=0.7):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for idx, row in df[df['topic_number'].astype(str).str.endswith('_-1')].iterrows():\n",
    "        emb = embeddings[idx].reshape(1,-1)\n",
    "        \n",
    "        similarities = {\n",
    "            topic: cosine_similarity(emb, centroid.reshape(1,-1))[0][0]\n",
    "            for topic, centroid in centroids.items()\n",
    "        }\n",
    "        \n",
    "        best_topic, best_score = max(similarities.items(), key=lambda x: x[1])\n",
    "        if best_score >= similarity_threshold:\n",
    "            df.at[idx, 'topic_number'] = best_topic\n",
    "            label_match = df[df['topic_number'] == best_topic]['topic_label']\n",
    "            if not label_match.empty:\n",
    "                df.at[idx, 'topic_label'] = label_match.iloc[0]\n",
    "\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de15371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update merged_df dataframe with new labels\n",
    "\n",
    "merged_df['topic_number'] = None\n",
    "merged_df['topic_label'] = None\n",
    "\n",
    "for result in results.values():\n",
    "    cluster = result['cluster']\n",
    "    topic_ids = result['topics']\n",
    "    model = result['model']\n",
    "    \n",
    "    # Get all paragraph indexes for cluster\n",
    "    cluster_indices = merged_df[merged_df['cluster'] == cluster].index\n",
    "    \n",
    "    # Assign topic # and label to each paragraph\n",
    "    for i, idx in enumerate(cluster_indices):\n",
    "        topic_id = topic_ids[i]\n",
    "        if topic_id == -1:\n",
    "            combined_id = f\"{cluster}_-1\"\n",
    "            label = \"Outlier\"\n",
    "        else:\n",
    "            combined_id = f\"{cluster}_{topic_id}\"\n",
    "            label = get_top_words(model, topic_id)\n",
    "            \n",
    "        merged_df.at[idx, 'topic_number'] = combined_id\n",
    "        merged_df.at[idx, \"topic_label\"] = label\n",
    "        \n",
    "# Check, total outliers should = 1739\n",
    "outliers_merged = len(merged_df[merged_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aceb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_merged = len(merged_df[merged_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers_merged}\")\n",
    "\n",
    "pct_outliers_merged = outliers_merged/len(merged_df)\n",
    "print(f\"Percent of outlier paragraphs: {pct_outliers_merged*100:.2f}%\")\n",
    "\n",
    "avg_corpus  = np.mean([r[\"coherence\"] for r in results.values()])\n",
    "print(f\"Average Coherence Across Clusters: {avg_corpus:.4f}\")\n",
    "\n",
    "global_topic_ids = set()\n",
    "\n",
    "for r in results.values():\n",
    "    model = r['model']\n",
    "    cluster_label = r['cluster']\n",
    "    for tid in model.get_topics().keys():\n",
    "        global_id = f\"cluster{cluster_label}_topic{tid}\"\n",
    "        global_topic_ids.add(global_id)\n",
    "    \n",
    "total_topics = len(global_topic_ids)\n",
    "print(f\"Total distinct topics across clusters: {total_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids = compute_per_cluster_centroids(merged_df, corpus_embeddings)\n",
    "cluster_reassigned_outliers_df = reassign_outliers_by_cluster(merged_df,\n",
    "                                                             corpus_embeddings,\n",
    "                                                             cluster_centroids,\n",
    "                                                             similarity_threshold=0.7)\n",
    "\n",
    "outliers = len(cluster_reassigned_outliers_df[cluster_reassigned_outliers_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers}\")\n",
    "\n",
    "pct_outliers = outliers/len(cluster_reassigned_outliers_df)\n",
    "print(f\"Percent of outlier paragraphs: {pct_outliers*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids = compute_per_cluster_centroids(merged_df, corpus_embeddings)\n",
    "\n",
    "cluster_reassigned_outliers_df = reassign_outliers_by_cluster(merged_df,\n",
    "                                                             corpus_embeddings,\n",
    "                                                             cluster_centroids,\n",
    "                                                             similarity_threshold=0.5)\n",
    "\n",
    "outliers = len(cluster_reassigned_outliers_df[cluster_reassigned_outliers_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers}\")\n",
    "\n",
    "pct_outliers = outliers/len(cluster_reassigned_outliers_df)\n",
    "print(f\"Percent of outlier paragraphs: {pct_outliers*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_centroids = compute_global_centroids(merged_df, corpus_embeddings)\n",
    "global_reassigned_outliers_df = reassign_outliers_globally(merged_df,\n",
    "                                                             corpus_embeddings,\n",
    "                                                             global_centroids,\n",
    "                                                             similarity_threshold=0.7)\n",
    "\n",
    "outliers = len(global_reassigned_outliers_df[global_reassigned_outliers_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers}\")\n",
    "\n",
    "pct_outliers = outliers/len(global_reassigned_outliers_df)\n",
    "print(f\"Percent of outlier paragraphs: {pct_outliers*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_centroids = compute_global_centroids(merged_df, corpus_embeddings)\n",
    "\n",
    "global_reassigned_outliers_df = reassign_outliers_globally(merged_df,\n",
    "                                                             corpus_embeddings,\n",
    "                                                             global_centroids,\n",
    "                                                             similarity_threshold=0.5)\n",
    "\n",
    "outliers = len(global_reassigned_outliers_df[global_reassigned_outliers_df['topic_label']==\"Outlier\"])\n",
    "print(f\"Total Outliers: {outliers}\")\n",
    "\n",
    "pct_outliers = outliers/len(global_reassigned_outliers_df)\n",
    "print(f\"Percent of outlier paragraphs: {pct_outliers*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e89dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading original dataframe from .db for subtopic to doc assignment\n",
    "con = sqlite3.connect(\"clustered_diseases.db\")\n",
    "\n",
    "doc_df = pd.read_sql_query(\"SELECT * from clustered_diseases\", con)\n",
    "con.close()\n",
    "\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any remaining outliers\n",
    "global_reassigned_removed_outliers_df = global_reassigned_outliers_df[global_reassigned_outliers_df['topic_number'].str.split('_').str[1] != '-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = global_reassigned_removed_outliers_df.groupby(['title', 'topic_number']).size().reset_index(name='topic_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef06199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any remaining outliers - By Cluster DF\n",
    "cluster_reassigned_removed_outliers_df = cluster_reassigned_outliers_df[cluster_reassigned_outliers_df['topic_number'].str.split('_').str[1] != '-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Counts by Cluster\n",
    "topic_counts = cluster_reassigned_removed_outliers_df.groupby(['title', 'topic_number']).size().reset_index(name='topic_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_to_topics = topic_counts.groupby('title')['topic_number'].apply(list).reset_index(name='topic_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c871a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = doc_df.merge(title_to_topics, on='title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adb650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the number of subtopics (length of topic_list) as a new column\n",
    "doc_df['num_subtopics'] = doc_df['topic_list'].apply(len)\n",
    "\n",
    "doc_df.sort_values(by='num_subtopics', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5d780",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(doc_df['num_subtopics'], bins=range(doc_df['num_subtopics'].max() + 2),\n",
    "        align='left', edgecolor='black')\n",
    "plt.title('Distribution of Number of Subtopics per Document')\n",
    "plt.xlabel('Number of Subtopics')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(range(doc_df['num_subtopics'].max() + 1))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist to db\n",
    "import json\n",
    "\n",
    "#Remove lists\n",
    "doc_df_str = doc_df.copy()\n",
    "doc_df_str['topic_list_str'] = doc_df_str['topic_list'].apply(json.dumps)\n",
    "doc_df_str.drop(columns=['topic_list'], inplace=True)\n",
    "doc_df_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving doc df with subtopic labels\n",
    "sql = sqlite3.connect(\"docs_with_subtopics.db\")\n",
    "doc_df_str.to_sql(\"docs_with_subtopics\", sql, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7859c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving topic count df\n",
    "\n",
    "sql = sqlite3.connect(\"topic_counts.db\")\n",
    "topic_counts.to_sql(\"topic_counts\", sql, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648f9c",
   "metadata": {},
   "source": [
    "## Creating Subtopic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "subtopic_representations = {}\n",
    "\n",
    "for cluster_id, result in results.items():\n",
    "    topic_model = result['model']\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    for topic_id in topic_info['Topic']:\n",
    "        if topic_id == -1:\n",
    "            continue\n",
    "            \n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        if not words:\n",
    "            continue\n",
    "            \n",
    "        rep_str = \" \".join([word for word, _ in words[:10]])\n",
    "        \n",
    "        subtopic_key = f\"{cluster_id}_{topic_id}\"\n",
    "        \n",
    "        subtopic_representations[subtopic_key] = rep_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_texts = list(subtopic_representations.values())\n",
    "\n",
    "rep_embeddings = model.encode(rep_texts, convert_to_tensor=True)\n",
    "\n",
    "subtopic_embeddings = {\n",
    "    sub_id: emb\n",
    "    for sub_id, emb in zip(subtopic_representations.keys(), rep_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_df = pd.DataFrame({\n",
    "    \"subtopic_id\": list(subtopic_representations.keys()),\n",
    "    \"representation\": list(subtopic_representations.values())\n",
    "})\n",
    "\n",
    "subtopic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab605d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subtopic_embeddings_NoMRR.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subtopic_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2326db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
